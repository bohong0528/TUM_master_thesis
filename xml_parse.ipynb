{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-02T15:19:01.084165900Z",
     "start_time": "2024-01-02T15:18:40.608334100Z"
    }
   },
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "# Load and parse the XML file\n",
    "tree = ET.parse('Scenarios/cutoutWorlds/po-1_pn-1.0_sn-1/plans.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "# Prepare a list to collect the activity data\n",
    "activities_data = []\n",
    "\n",
    "# Iterate through each person in the XML\n",
    "for person in root.findall('person'):\n",
    "    person_id = person.get('id')\n",
    "    \n",
    "    # Collect each activity's data for the person\n",
    "    for activity in person.findall('.//activity'):\n",
    "        activity_type = activity.get('type')\n",
    "        link = activity.get('link')\n",
    "        # Split the activity type into two parts\n",
    "        if '_' in activity_type:\n",
    "            activity_type_main, activity_type_value = activity_type.split('_')\n",
    "        else:\n",
    "            activity_type_main, activity_type_value = activity_type, 0    \n",
    "        # Create a dictionary for each activity of type \"other\"\n",
    "        try:\n",
    "            activity_data = {\n",
    "                'person_id': person_id,\n",
    "                'activity_type_main': activity_type_main,\n",
    "                'activity_type_value': float(activity_type_value),  # Convert to float\n",
    "                'link': link,\n",
    "                'x': float(activity.get('x')),  # Convert to float\n",
    "                'y': float(activity.get('y'))   # Convert to float\n",
    "            }\n",
    "        except:\n",
    "            print(\"Error parsing\")\n",
    "            print(activity_type_value)\n",
    "            activity_data ={}\n",
    "        # Add the activity data to the list\n",
    "        activities_data.append(activity_data)\n",
    "\n",
    "# Convert the list of data into a pandas DataFrame\n",
    "df_activities = pd.DataFrame(activities_data)\n",
    "df_activities.to_pickle(\"./df_activities.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load and parse the XML file\n",
    "tree = ET.parse('Scenarios/cutoutWorlds/po-1_pn-1.0_sn-1/network.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "# Prepare a list to collect the activity data\n",
    "links_data = []\n",
    "nodes_data = []\n",
    "# Iterate through each person in the XML\n",
    "for nodes in root.findall('nodes'): \n",
    "    for node in nodes.findall('.//node'):\n",
    "        node_id = node.get('id')\n",
    "        node_data = {\n",
    "                'node_id': node_id,\n",
    "                'x': float(node.get('x')),  # Convert to float\n",
    "                'y': float(node.get('y'))   # Convert to float\n",
    "        }\n",
    "        nodes_data.append(node_data)\n",
    "df_nodes = pd.DataFrame(nodes_data)\n",
    "\n",
    "for links in root.findall('links'): \n",
    "    for link in links.findall('.//link'):\n",
    "        link_id = link.get('id')\n",
    "        link_data = {\n",
    "                'link_id': link_id,\n",
    "                'from': link.get('from'),  # Convert to float\n",
    "                'to': link.get('to'),\n",
    "                'length': float(link.get('length')),\n",
    "                'freespeed': float(link.get('freespeed')),\n",
    "                'capacity': float(link.get('capacity')),\n",
    "                'permlanes': float(link.get('permlanes'))\n",
    "        }\n",
    "        links_data.append(link_data)\n",
    "df_links = pd.DataFrame(links_data)\n",
    "df_links = df_links.merge(df_nodes, how='left', left_on='from', right_on='node_id')\n",
    "df_links = df_links.rename(columns={'x': 'start_node_x', 'y': 'start_node_y'})\n",
    "df_links.drop('node_id', axis=1, inplace=True)\n",
    "df_links = df_links.merge(df_nodes, how='left', left_on='to', right_on='node_id')\n",
    "df_links = df_links.rename(columns={'x': 'end_node_x', 'y': 'end_node_y'})\n",
    "df_links.drop('node_id', axis=1, inplace=True)\n",
    "df_links.to_pickle(\"./df_links_network.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T15:19:11.269842800Z",
     "start_time": "2024-01-02T15:19:01.095714300Z"
    }
   },
   "id": "7ef72846d1ffca0a",
   "execution_count": 80
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "new = df_activities.merge(df_links, how='left', left_on='link', right_on='link_id')\n",
    "new.drop(['length','freespeed','capacity','permlanes'], axis=1, inplace=True)\n",
    "new"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23b63ccf47341986",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import json\n",
    "file = 'Data/cutoutWorlds/Train/po-1_pn-1.0_sn-1/s-0.json'\n",
    "with open(file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "df_work = pd.DataFrame({\n",
    "    'work_x': data['work_x'],\n",
    "    'work_y': data['work_y'],\n",
    "    'go_to_work': data['go_to_work']\n",
    "})\n",
    "df_home = pd.DataFrame({\n",
    "    'home_x': data['home_x'],\n",
    "    'home_y': data['home_y'],\n",
    "    'go_to_home': data['go_to_home']\n",
    "})\n",
    "\n",
    "df_links_dataset = pd.DataFrame({\n",
    "                'link_id': data['links_id'],\n",
    "                'link_from': data['link_from'],\n",
    "                'link_to': data['link_to'],\n",
    "                'link_length': data['link_length']\n",
    "            })\n",
    "df_nodes_dataset = pd.DataFrame({\n",
    "                'node_id': data['nodes_id'],\n",
    "                'node_x': data['nodes_x'],\n",
    "                'node_y': data['nodes_y']\n",
    "            })\n",
    "df_links_dataset = df_links_dataset.merge(df_nodes_dataset, how='left', left_on='link_from', right_on='node_id')\n",
    "df_links_dataset = df_links_dataset.rename(columns={'node_x': 'start_node_x', 'node_y': 'start_node_y'})\n",
    "df_links_dataset.drop('node_id', axis=1, inplace=True)\n",
    "df_links_dataset = df_links_dataset.merge(df_nodes_dataset, how='left', left_on='link_to', right_on='node_id')\n",
    "df_links_dataset = df_links_dataset.rename(columns={'node_x': 'end_node_x', 'node_y': 'end_node_y'})\n",
    "df_links_dataset.drop('node_id', axis=1, inplace=True) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ae81199ce4b5c85",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_act_work = df_activities[df_activities['activity_type_main']=='work']\n",
    "df_act_work = df_act_work.merge(df_work, how='left', left_on=['x','y'], right_on=['work_x','work_y'])\n",
    "df_act_work.drop(['x','y'], axis=1, inplace=True)\n",
    "df_act_work_agg = df_act_work.groupby(by=\"link\").sum()['go_to_work'].reset_index(drop=False)\n",
    "df_act_home = df_activities[df_activities['activity_type_main']=='home']\n",
    "df_act_home = df_act_home.merge(df_home, how='left', left_on=['x','y'], right_on=['home_x','home_y'])\n",
    "df_act_home.drop(['x','y'], axis=1, inplace=True)\n",
    "df_act_home_agg = df_act_home.groupby(by=\"link\").sum()['go_to_home'].reset_index(drop=False)\n",
    "df_act_agg = df_act_home_agg.merge(df_act_work_agg, how='outer', on='link')\n",
    "df_act_agg.fillna(0, inplace=True)\n",
    "df_act_agg['go_to_sum'] = df_act_agg['go_to_home'] + df_act_agg['go_to_work']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b29e2c7f17cd46e9",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "mg = df_links_dataset.merge(df_links, how='left', on=['start_node_x','start_node_y','end_node_x','end_node_y'])\n",
    "mg = mg[['link_id_x','link_from','link_to','link_id_y','from', 'to']]\n",
    "mg = mg.rename(columns={'link_id_x': 'link_id_dataset', 'link_id_y': 'link_id_network', 'link_from': 'node_from_dataset', 'from': 'node_from_network','link_to': 'node_to_dataset', 'to': 'node_to_network'})"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "606ef910855e5933",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "link_home_work = mg.merge(df_act_agg, how='left', left_on='link_id_network', right_on='link')\n",
    "link_home_work['go_to_sum'].fillna(0, inplace=True)\n",
    "link_go_to = link_home_work[['link_id_dataset', 'go_to_sum']]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7310051927463f99",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7d964a5bd8878f9d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ece8ba9d92101ea5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
