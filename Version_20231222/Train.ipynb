{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T19:49:39.680056800Z",
     "start_time": "2024-01-02T19:49:36.808325500Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold, RandomizedSearchCV\n",
    "from scipy.stats import expon, reciprocal, uniform\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel, DotProduct\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T19:49:55.184659400Z",
     "start_time": "2024-01-02T19:49:39.702704300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:56: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_work_agg = df_act_work.groupby(by=\"link\").sum()['go_to_work'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:60: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_home_agg = df_act_home.groupby(by=\"link\").sum()['go_to_home'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:56: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_work_agg = df_act_work.groupby(by=\"link\").sum()['go_to_work'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:60: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_home_agg = df_act_home.groupby(by=\"link\").sum()['go_to_home'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:56: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_work_agg = df_act_work.groupby(by=\"link\").sum()['go_to_work'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:60: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_home_agg = df_act_home.groupby(by=\"link\").sum()['go_to_home'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:56: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_work_agg = df_act_work.groupby(by=\"link\").sum()['go_to_work'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:60: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_home_agg = df_act_home.groupby(by=\"link\").sum()['go_to_home'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:56: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_work_agg = df_act_work.groupby(by=\"link\").sum()['go_to_work'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:60: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_home_agg = df_act_home.groupby(by=\"link\").sum()['go_to_home'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:56: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_work_agg = df_act_work.groupby(by=\"link\").sum()['go_to_work'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:60: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_home_agg = df_act_home.groupby(by=\"link\").sum()['go_to_home'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:56: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_work_agg = df_act_work.groupby(by=\"link\").sum()['go_to_work'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:60: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_home_agg = df_act_home.groupby(by=\"link\").sum()['go_to_home'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:56: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_work_agg = df_act_work.groupby(by=\"link\").sum()['go_to_work'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:60: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_home_agg = df_act_home.groupby(by=\"link\").sum()['go_to_home'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:56: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_work_agg = df_act_work.groupby(by=\"link\").sum()['go_to_work'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:60: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_home_agg = df_act_home.groupby(by=\"link\").sum()['go_to_home'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:56: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_work_agg = df_act_work.groupby(by=\"link\").sum()['go_to_work'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:60: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_home_agg = df_act_home.groupby(by=\"link\").sum()['go_to_home'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:56: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_work_agg = df_act_work.groupby(by=\"link\").sum()['go_to_work'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:60: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_home_agg = df_act_home.groupby(by=\"link\").sum()['go_to_home'].reset_index(drop=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:56: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_work_agg = df_act_work.groupby(by=\"link\").sum()['go_to_work'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:60: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_home_agg = df_act_home.groupby(by=\"link\").sum()['go_to_home'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:56: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_work_agg = df_act_work.groupby(by=\"link\").sum()['go_to_work'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:60: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_home_agg = df_act_home.groupby(by=\"link\").sum()['go_to_home'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:56: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_work_agg = df_act_work.groupby(by=\"link\").sum()['go_to_work'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:60: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_home_agg = df_act_home.groupby(by=\"link\").sum()['go_to_home'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:56: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_work_agg = df_act_work.groupby(by=\"link\").sum()['go_to_work'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:60: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_home_agg = df_act_home.groupby(by=\"link\").sum()['go_to_home'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:56: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_work_agg = df_act_work.groupby(by=\"link\").sum()['go_to_work'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:60: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_home_agg = df_act_home.groupby(by=\"link\").sum()['go_to_home'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:56: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_work_agg = df_act_work.groupby(by=\"link\").sum()['go_to_work'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:60: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_home_agg = df_act_home.groupby(by=\"link\").sum()['go_to_home'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:56: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_work_agg = df_act_work.groupby(by=\"link\").sum()['go_to_work'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:60: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_home_agg = df_act_home.groupby(by=\"link\").sum()['go_to_home'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:56: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_work_agg = df_act_work.groupby(by=\"link\").sum()['go_to_work'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:60: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_home_agg = df_act_home.groupby(by=\"link\").sum()['go_to_home'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:56: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_work_agg = df_act_work.groupby(by=\"link\").sum()['go_to_work'].reset_index(drop=False)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_12584\\2278236610.py:60: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  df_act_home_agg = df_act_home.groupby(by=\"link\").sum()['go_to_home'].reset_index(drop=False)\n"
     ]
    }
   ],
   "source": [
    "def load_data(file_list, df_activities, df_links_network):\n",
    "    data_frames = []\n",
    "    for file in file_list:\n",
    "        with open(file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            df_links = pd.DataFrame({\n",
    "                'link_id': data['links_id'],\n",
    "                'link_from': data['link_from'],\n",
    "                'link_to': data['link_to'],\n",
    "                'link_length': data['link_length'],\n",
    "                'link_freespeed': data['link_freespeed'],\n",
    "                'link_capacity': data['link_capacity'],\n",
    "                'link_permlanes': data['link_permlanes'],\n",
    "                'link_counts': data['link_counts']\n",
    "            })\n",
    "            df_nodes = pd.DataFrame({\n",
    "                'node_id': data['nodes_id'],\n",
    "                'node_x': data['nodes_x'],\n",
    "                'node_y': data['nodes_y']\n",
    "            })\n",
    "            df_od_pairs = pd.DataFrame(data['o_d_pairs'], columns=['origin', 'destination'])\n",
    "            \n",
    "            df_work = pd.DataFrame({\n",
    "                        'work_x': data['work_x'],\n",
    "                        'work_y': data['work_y'],\n",
    "                        'go_to_work': data['go_to_work']\n",
    "            })\n",
    "            df_home = pd.DataFrame({\n",
    "                'home_x': data['home_x'],\n",
    "                'home_y': data['home_y'],\n",
    "                'go_to_home': data['go_to_home']\n",
    "            })\n",
    "            \n",
    "            df_links = df_links.merge(df_nodes, how='left', left_on='link_from', right_on='node_id')\n",
    "            df_links = df_links.rename(columns={'node_x': 'start_node_x', 'node_y': 'start_node_y'})\n",
    "            df_links.drop('node_id', axis=1, inplace=True)\n",
    "            df_links = df_links.merge(df_nodes, how='left', left_on='link_to', right_on='node_id')\n",
    "            df_links = df_links.rename(columns={'node_x': 'end_node_x', 'node_y': 'end_node_y'})\n",
    "            df_links.drop('node_id', axis=1, inplace=True) \n",
    "            \n",
    "            origin_counts = df_od_pairs['origin'].value_counts()\n",
    "            df_origin_counts = origin_counts.reset_index()\n",
    "            df_origin_counts.columns = ['origin', 'start_count']\n",
    "            destination_counts = df_od_pairs['destination'].value_counts()\n",
    "            df_destination_counts = destination_counts.reset_index()\n",
    "            df_destination_counts.columns = ['destination', 'end_count']\n",
    "            df_links = df_links.merge(df_origin_counts, how='left', left_on='link_from', right_on='origin')\n",
    "            df_links.drop('origin', axis=1, inplace=True)\n",
    "            df_links = df_links.merge(df_destination_counts, how='left', left_on='link_to', right_on='destination')\n",
    "            df_links.drop('destination', axis=1, inplace=True)\n",
    "            df_links[['start_count','end_count']] = df_links[['start_count','end_count']].fillna(0)\n",
    "            \n",
    "            df_act_work = df_activities[df_activities['activity_type_main']=='work']\n",
    "            df_act_work = df_act_work.merge(df_work, how='left', left_on=['x','y'], right_on=['work_x','work_y'])\n",
    "            df_act_work.drop(['x','y'], axis=1, inplace=True)\n",
    "            df_act_work_agg = df_act_work.groupby(by=\"link\").sum()['go_to_work'].reset_index(drop=False)\n",
    "            df_act_home = df_activities[df_activities['activity_type_main']=='home']\n",
    "            df_act_home = df_act_home.merge(df_home, how='left', left_on=['x','y'], right_on=['home_x','home_y'])\n",
    "            df_act_home.drop(['x','y'], axis=1, inplace=True)\n",
    "            df_act_home_agg = df_act_home.groupby(by=\"link\").sum()['go_to_home'].reset_index(drop=False)\n",
    "            df_act_agg = df_act_home_agg.merge(df_act_work_agg, how='outer', on='link')\n",
    "            df_act_agg.fillna(0, inplace=True)\n",
    "            df_act_agg['go_to_sum'] = df_act_agg['go_to_home'] + df_act_agg['go_to_work']\n",
    "            \n",
    "            mg = df_links.merge(df_links_network, how='left', on=['start_node_x','start_node_y','end_node_x','end_node_y'])\n",
    "            mg = mg[['link_id_x','link_from','link_to','link_id_y','from', 'to']]\n",
    "            link_home_work = mg.merge(df_act_agg, how='left', left_on='link_id_y', right_on='link')\n",
    "            link_home_work['go_to_sum'].fillna(0, inplace=True)\n",
    "            link_go_to = link_home_work[['link_id_x', 'go_to_sum']]\n",
    "            df_links = df_links.merge(link_go_to, how='left', left_on='link_id', right_on='link_id_x')\n",
    "            df_links.drop('link_id_x', axis=1, inplace=True)\n",
    "        data_frames.append(df_links)\n",
    "    return pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "train_files = ['s-0.json', 's-1.json', 's-2.json', 's-3.json', 's-4.json','s-5.json', 's-6.json', 's-7.json', 's-8.json', 's-9.json'] \n",
    "test_files = ['s-15.json', 's-16.json', 's-17.json', 's-18.json','s-19.json']\n",
    "validate_files = ['s-10.json', 's-11.json', 's-12.json', 's-13.json','s-14.json']\n",
    "train_files = ['Data/cutoutWorlds/Train/po-1_pn-1.0_sn-1/' + i for i in train_files]\n",
    "test_files = ['Data/cutoutWorlds/Test/po-1_pn-1.0_sn-1/' + j for j in test_files]\n",
    "validate_files = ['Data/cutoutWorlds/Validate/po-1_pn-1.0_sn-1/' + k for k in validate_files]\n",
    "df_activities = pd.read_pickle(\"Data/cutoutWorlds/Train/po-1_pn-1.0_sn-1/df_activities.pkl\")\n",
    "df_links_network = pd.read_pickle(\"Data/cutoutWorlds/Train/po-1_pn-1.0_sn-1/df_links_network.pkl\")\n",
    "train_data = load_data(train_files, df_activities, df_links_network)\n",
    "test_data = load_data(test_files, df_activities, df_links_network)\n",
    "validate_data = load_data(validate_files, df_activities, df_links_network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T20:01:05.026429300Z",
     "start_time": "2023-12-29T20:01:04.818365100Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Initialize a list to hold trips\n",
    "# trips = []\n",
    "# current_trip = [df_od_pairs.iloc[0]['origin']]  # Start with the first origin\n",
    "# \n",
    "# # Iterate over the DataFrame rows\n",
    "# for i, row in df_od_pairs.iterrows():\n",
    "#     current_trip.append(row['destination'])  # Always add the destination\n",
    "#     # Check if the next origin matches the current destination\n",
    "#     if i + 1 < len(df_od_pairs) and row['destination'] != df_od_pairs.iloc[i + 1]['origin']:\n",
    "#         # If it doesn't, the current trip has ended\n",
    "#         trips.append(current_trip)\n",
    "#         current_trip = [df_od_pairs.iloc[i + 1]['origin']]  # Start a new trip\n",
    "# \n",
    "# # Add the last trip if it wasn't already added\n",
    "# if current_trip not in trips:\n",
    "#     trips.append(current_trip)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T20:01:05.028445Z",
     "start_time": "2023-12-29T20:01:04.975096700Z"
    }
   },
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# # Flatten the list of trips into a single list of nodes including origins and destinations\n",
    "# all_nodes = [node for trip in trips for node in trip]\n",
    "# \n",
    "# # Use Counter to count the occurrences of each node\n",
    "# node_trip_counts = Counter(all_nodes)\n",
    "# \n",
    "# df_node_trip_counts = pd.DataFrame.from_dict(node_trip_counts, orient='index').reset_index()\n",
    "# df_node_trip_counts.columns = ['node_id', 'trip_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T15:44:41.533491700Z",
     "start_time": "2024-01-02T15:44:41.482248800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_id</th>\n",
       "      <th>link_from</th>\n",
       "      <th>link_to</th>\n",
       "      <th>link_length</th>\n",
       "      <th>link_freespeed</th>\n",
       "      <th>link_capacity</th>\n",
       "      <th>link_permlanes</th>\n",
       "      <th>link_counts</th>\n",
       "      <th>start_node_x</th>\n",
       "      <th>start_node_y</th>\n",
       "      <th>end_node_x</th>\n",
       "      <th>end_node_y</th>\n",
       "      <th>start_count</th>\n",
       "      <th>end_count</th>\n",
       "      <th>go_to_sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>425</td>\n",
       "      <td>579</td>\n",
       "      <td>134.962910</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.609957e+06</td>\n",
       "      <td>5.819853e+06</td>\n",
       "      <td>4.609956e+06</td>\n",
       "      <td>5.819988e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>579</td>\n",
       "      <td>425</td>\n",
       "      <td>134.962910</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>300.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.609956e+06</td>\n",
       "      <td>5.819988e+06</td>\n",
       "      <td>4.609957e+06</td>\n",
       "      <td>5.819853e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>524</td>\n",
       "      <td>620</td>\n",
       "      <td>49.508163</td>\n",
       "      <td>6.944444</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>27.0</td>\n",
       "      <td>4.614751e+06</td>\n",
       "      <td>5.819976e+06</td>\n",
       "      <td>4.614750e+06</td>\n",
       "      <td>5.820025e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>620</td>\n",
       "      <td>524</td>\n",
       "      <td>49.508163</td>\n",
       "      <td>6.944444</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>4.614750e+06</td>\n",
       "      <td>5.820025e+06</td>\n",
       "      <td>4.614751e+06</td>\n",
       "      <td>5.819976e+06</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>656</td>\n",
       "      <td>652</td>\n",
       "      <td>13.326026</td>\n",
       "      <td>6.944444</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>4.615677e+06</td>\n",
       "      <td>5.819981e+06</td>\n",
       "      <td>4.615681e+06</td>\n",
       "      <td>5.819993e+06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16980</th>\n",
       "      <td>1391</td>\n",
       "      <td>212</td>\n",
       "      <td>704</td>\n",
       "      <td>1124.319428</td>\n",
       "      <td>22.222222</td>\n",
       "      <td>600.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>4.624410e+06</td>\n",
       "      <td>5.875182e+06</td>\n",
       "      <td>4.623684e+06</td>\n",
       "      <td>5.874330e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16981</th>\n",
       "      <td>1392</td>\n",
       "      <td>678</td>\n",
       "      <td>679</td>\n",
       "      <td>43.946365</td>\n",
       "      <td>22.222222</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>36.0</td>\n",
       "      <td>4.643553e+06</td>\n",
       "      <td>5.884511e+06</td>\n",
       "      <td>4.643596e+06</td>\n",
       "      <td>5.884518e+06</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16982</th>\n",
       "      <td>1393</td>\n",
       "      <td>679</td>\n",
       "      <td>678</td>\n",
       "      <td>43.946365</td>\n",
       "      <td>22.222222</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>33.0</td>\n",
       "      <td>4.643596e+06</td>\n",
       "      <td>5.884518e+06</td>\n",
       "      <td>4.643553e+06</td>\n",
       "      <td>5.884511e+06</td>\n",
       "      <td>2.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16983</th>\n",
       "      <td>1394</td>\n",
       "      <td>679</td>\n",
       "      <td>412</td>\n",
       "      <td>42.684073</td>\n",
       "      <td>22.222222</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>35.0</td>\n",
       "      <td>4.643596e+06</td>\n",
       "      <td>5.884518e+06</td>\n",
       "      <td>4.643639e+06</td>\n",
       "      <td>5.884524e+06</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16984</th>\n",
       "      <td>1395</td>\n",
       "      <td>412</td>\n",
       "      <td>679</td>\n",
       "      <td>42.684073</td>\n",
       "      <td>22.222222</td>\n",
       "      <td>6000.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>32.0</td>\n",
       "      <td>4.643639e+06</td>\n",
       "      <td>5.884524e+06</td>\n",
       "      <td>4.643596e+06</td>\n",
       "      <td>5.884518e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16985 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       link_id  link_from  link_to  link_length  link_freespeed  \\\n",
       "0            0        425      579   134.962910        4.166667   \n",
       "1            1        579      425   134.962910        4.166667   \n",
       "2            2        524      620    49.508163        6.944444   \n",
       "3            3        620      524    49.508163        6.944444   \n",
       "4            4        656      652    13.326026        6.944444   \n",
       "...        ...        ...      ...          ...             ...   \n",
       "16980     1391        212      704  1124.319428       22.222222   \n",
       "16981     1392        678      679    43.946365       22.222222   \n",
       "16982     1393        679      678    43.946365       22.222222   \n",
       "16983     1394        679      412    42.684073       22.222222   \n",
       "16984     1395        412      679    42.684073       22.222222   \n",
       "\n",
       "       link_capacity  link_permlanes  link_counts  start_node_x  start_node_y  \\\n",
       "0              300.0             0.5          0.0  4.609957e+06  5.819853e+06   \n",
       "1              300.0             0.5          0.0  4.609956e+06  5.819988e+06   \n",
       "2             1800.0             1.5         27.0  4.614751e+06  5.819976e+06   \n",
       "3             2400.0             2.0         35.0  4.614750e+06  5.820025e+06   \n",
       "4             1600.0             1.0         79.0  4.615677e+06  5.819981e+06   \n",
       "...              ...             ...          ...           ...           ...   \n",
       "16980          600.0             1.0         10.0  4.624410e+06  5.875182e+06   \n",
       "16981         6000.0             1.5         36.0  4.643553e+06  5.884511e+06   \n",
       "16982         6000.0             1.5         33.0  4.643596e+06  5.884518e+06   \n",
       "16983         6000.0             1.5         35.0  4.643596e+06  5.884518e+06   \n",
       "16984         6000.0             1.5         32.0  4.643639e+06  5.884524e+06   \n",
       "\n",
       "         end_node_x    end_node_y  start_count  end_count  go_to_sum  \n",
       "0      4.609956e+06  5.819988e+06          0.0        0.0        0.0  \n",
       "1      4.609957e+06  5.819853e+06          0.0        0.0        0.0  \n",
       "2      4.614750e+06  5.820025e+06          0.0        2.0        0.0  \n",
       "3      4.614751e+06  5.819976e+06          2.0        0.0        0.0  \n",
       "4      4.615681e+06  5.819993e+06          1.0        9.0        0.0  \n",
       "...             ...           ...          ...        ...        ...  \n",
       "16980  4.623684e+06  5.874330e+06          0.0        0.0        0.0  \n",
       "16981  4.643596e+06  5.884518e+06         13.0        2.0        0.0  \n",
       "16982  4.643553e+06  5.884511e+06          2.0       13.0        0.0  \n",
       "16983  4.643639e+06  5.884524e+06          2.0        0.0        0.0  \n",
       "16984  4.643596e+06  5.884518e+06          0.0        2.0        0.0  \n",
       "\n",
       "[16985 rows x 15 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T19:49:55.235792300Z",
     "start_time": "2024-01-02T19:49:55.191146500Z"
    }
   },
   "outputs": [],
   "source": [
    "numerical_features = ['link_length', 'link_freespeed', 'link_capacity', 'link_permlanes', 'start_count', 'end_count', 'go_to_sum']\n",
    "X_t = train_data.drop(columns=['link_counts'])\n",
    "y_t = train_data['link_counts']\n",
    "X_v = validate_data.drop(columns=['link_counts'])\n",
    "y_v = validate_data['link_counts']\n",
    "scaler = StandardScaler()\n",
    "X_t[numerical_features] = scaler.fit_transform(X_t[numerical_features])\n",
    "X_v[numerical_features] = scaler.fit_transform(X_v[numerical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T15:58:27.133238400Z",
     "start_time": "2024-01-02T15:58:16.618137100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "#     'Linear Regression': LinearRegression(),\n",
    "#     'Lasso': LassoCV(cv=3, random_state=42, max_iter=100000),\n",
    "#     'Ridge': RidgeCV(cv=3),\n",
    "#     'SVR': SVR(C=25.383309585489613, epsilon=0.01, gamma=2.1969677491639246, max_iter=2000),\n",
    "#     'Random Forest': RandomForestRegressor(criterion='friedman_mse', max_depth=20, min_samples_leaf=2, n_estimators=150, random_state=42),\n",
    "#     'Gradient Boosting': GradientBoostingRegressor(max_depth=5, min_samples_split=5, random_state=42, subsample=0.8),\n",
    "#     'Artificial Neural Network': MLPRegressor(activation='tanh', alpha=0.001, learning_rate_init=0.01, max_iter=1000, random_state=42, solver='sgd'),\n",
    "#     'Gaussian Process Regression_none': GaussianProcessRegressor(),\n",
    "    'Gaussian Process Regression': GaussianProcessRegressor(kernel=50.0**2 * RBF(length_scale=50.0))\n",
    "}\n",
    "\n",
    "# Function to train and evaluate models\n",
    "def evaluate_models(models, X_train, y_train, X_test, y_test):\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "#         kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "#         cv_scores = cross_val_score(model, X_train, y_train, cv=kf, scoring='neg_mean_squared_error')\n",
    "#         mse_scores = -cv_scores \n",
    "#         mean_mse = mse_scores.mean()\n",
    "#         std_mse = mse_scores.std()\n",
    "        # mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "        # r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        \n",
    "        results[name] = {'MAE': mae, 'MSE': mse}\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Train and evaluate\n",
    "results = evaluate_models(models, X_t, y_t, X_v, y_v)\n",
    "\n",
    "# Display Results\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"{metric_name}: {value}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T15:58:29.829296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 70 candidates, totalling 210 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [-2.06753800e+21 -1.78347029e+03 -3.21274664e+21 -4.52036712e+58\n",
      " -2.11726699e+03 -1.78346984e+03 -1.78347029e+03 -1.78347029e+03\n",
      "             nan             nan -6.24658420e+19 -6.10801167e+12\n",
      " -1.96408398e+16 -1.78347029e+03             nan -1.78347029e+03\n",
      " -1.78347029e+03 -5.64219848e+20             nan -5.87544447e+21\n",
      "             nan -1.78347029e+03 -1.78347029e+03             nan\n",
      " -1.78347029e+03             nan -4.64578815e+15 -1.77140788e+03\n",
      "             nan -1.78347029e+03 -1.18411241e+16 -1.92220132e+03\n",
      " -1.78347029e+03 -1.78346995e+03             nan -2.11002684e+03\n",
      " -1.07400560e+17             nan             nan -1.78347029e+03\n",
      " -2.04274595e+03 -1.78347029e+03 -1.78347028e+03 -1.78347029e+03\n",
      " -1.78347029e+03 -1.78346908e+03 -1.77144765e+03 -1.78204420e+03\n",
      " -1.78347029e+03 -1.78347029e+03 -1.78347029e+03 -1.78070475e+03\n",
      " -2.11794703e+69 -1.78347029e+03 -1.78347029e+03 -2.47674823e+13\n",
      " -2.09596395e+22 -7.27577476e+20 -1.78347029e+03 -1.78347029e+03\n",
      " -5.24770013e+17 -2.66758547e+48             nan -8.75319761e+11\n",
      " -1.78347029e+03 -8.04400389e+14 -1.30787052e+23             nan\n",
      " -1.37463938e+10             nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 25.383309585489613, 'epsilon': 0.01, 'gamma': 2.1969677491639246, 'kernel': 'rbf'}\n",
      "SVR(C=25.383309585489613, epsilon=0.01, gamma=2.1969677491639246, max_iter=2000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=2000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "param_grid_svr = {\n",
    "    'C': reciprocal(1e-4, 1e3),  # Extended range for the regularization parameter\n",
    "    'gamma': reciprocal(1e-4, 1e2),  # Including specific gamma values\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],  # Focusing on RBF kernel\n",
    "    'epsilon': [0.01, 0.1, 0.2],  # Epsilon in the epsilon-SVR model\n",
    "}\n",
    "\n",
    "random_search_svr = RandomizedSearchCV(SVR(max_iter=2000), param_grid_svr, n_iter=70, cv=3, n_jobs=-1, verbose=10, scoring='neg_mean_squared_error')\n",
    "random_search_svr.fit(X_t, y_t)\n",
    "print(random_search_svr.best_params_)\n",
    "print(random_search_svr.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-31T01:20:56.486270700Z",
     "start_time": "2023-12-31T01:19:49.061059700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
      "{'criterion': 'friedman_mse', 'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 150}\n",
      "RandomForestRegressor(criterion='friedman_mse', max_depth=20,\n",
      "                      min_samples_leaf=2, n_estimators=150, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2,4],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'criterion':['friedman_mse']\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(RandomForestRegressor(random_state=42), param_grid_rf, cv=3, n_jobs=-1, verbose=10, scoring='neg_mean_squared_error')\n",
    "grid_search_rf.fit(X_t, y_t)\n",
    "print(grid_search_rf.best_params_)\n",
    "print(grid_search_rf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-31T14:55:59.094098400Z",
     "start_time": "2023-12-31T14:48:54.215646500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.8}\n",
      "GradientBoostingRegressor(max_depth=5, min_samples_split=5, random_state=42,\n",
      "                          subsample=0.8)\n"
     ]
    }
   ],
   "source": [
    "param_grid_gb = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],  # Varied learning rates for gradient boosting\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 4],\n",
    "    'subsample': [0.8, 1.0],  # Fraction of samples to be used for fitting individual base learners\n",
    "}\n",
    "\n",
    "grid_search_gb = GridSearchCV(GradientBoostingRegressor(random_state=42), param_grid_gb, cv=3, n_jobs=-1, verbose=5, scoring='neg_mean_squared_error')\n",
    "grid_search_gb.fit(X_t, y_t)\n",
    "print(grid_search_gb.best_params_)\n",
    "print(grid_search_gb.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T19:56:11.590867600Z",
     "start_time": "2024-01-02T19:50:33.301610200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 192 candidates, totalling 576 fits\n",
      "{'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'learning_rate_init': 0.01, 'solver': 'sgd'}\n",
      "MLPRegressor(activation='tanh', alpha=0.001, learning_rate_init=0.01,\n",
      "             max_iter=1000, random_state=42, solver='sgd')\n"
     ]
    }
   ],
   "source": [
    "param_grid_mlp = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.001, 0.01, 0.1],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "    'learning_rate_init': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "grid_search_mlp = GridSearchCV(MLPRegressor(max_iter=1000, random_state=42), param_grid_mlp, cv=3, n_jobs=-1, verbose=5, scoring='neg_mean_squared_error')\n",
    "grid_search_mlp.fit(X_t, y_t)\n",
    "print(grid_search_mlp.best_params_)\n",
    "print(grid_search_mlp.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-01T21:55:20.525741600Z",
     "start_time": "2024-01-01T20:54:36.357249300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "9 fits failed out of a total of 15.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 310, in fit\n",
      "    self.L_ = cholesky(K, lower=GPR_CHOLESKY_LOWER, check_finite=False)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: (\"The kernel, DotProduct(sigma_0=10), is not returning a positive definite matrix. Try gradually increasing the 'alpha' parameter of your GaussianProcessRegressor estimator.\", '8-th leading minor of the array is not positive definite')\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 310, in fit\n",
      "    self.L_ = cholesky(K, lower=GPR_CHOLESKY_LOWER, check_finite=False)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: (\"The kernel, DotProduct(sigma_0=10), is not returning a positive definite matrix. Try gradually increasing the 'alpha' parameter of your GaussianProcessRegressor estimator.\", '13-th leading minor of the array is not positive definite')\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 310, in fit\n",
      "    self.L_ = cholesky(K, lower=GPR_CHOLESKY_LOWER, check_finite=False)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: (\"The kernel, DotProduct(sigma_0=10), is not returning a positive definite matrix. Try gradually increasing the 'alpha' parameter of your GaussianProcessRegressor estimator.\", '17-th leading minor of the array is not positive definite')\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 310, in fit\n",
      "    self.L_ = cholesky(K, lower=GPR_CHOLESKY_LOWER, check_finite=False)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: (\"The kernel, DotProduct(sigma_0=0.1), is not returning a positive definite matrix. Try gradually increasing the 'alpha' parameter of your GaussianProcessRegressor estimator.\", '8-th leading minor of the array is not positive definite')\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 310, in fit\n",
      "    self.L_ = cholesky(K, lower=GPR_CHOLESKY_LOWER, check_finite=False)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: (\"The kernel, DotProduct(sigma_0=0.1), is not returning a positive definite matrix. Try gradually increasing the 'alpha' parameter of your GaussianProcessRegressor estimator.\", '13-th leading minor of the array is not positive definite')\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [           nan            nan -2092.47387429            nan\n",
      " -2083.41356604]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kernel': RBF(length_scale=10), 'alpha': 0.01}\n",
      "GaussianProcessRegressor(alpha=0.01, kernel=RBF(length_scale=10))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel, DotProduct\n",
    "import numpy as np\n",
    "param_grid = [{\n",
    "    \"alpha\":  [1e-2, 1e-3],\n",
    "    \"kernel\": [RBF(l) for l in np.logspace(-1, 1, 2)]\n",
    "}, {\n",
    "    \"alpha\":  [1e-2, 1e-3],\n",
    "    \"kernel\": [DotProduct(sigma_0) for sigma_0 in np.logspace(-1, 1, 2)]\n",
    "}]\n",
    "\n",
    "gpr = GaussianProcessRegressor()\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_gpr = RandomizedSearchCV(gpr, param_grid, n_iter=5, cv=3, scoring='neg_mean_squared_error', n_jobs=3, verbose=10)\n",
    "grid_search_gpr.fit(X_t, y_t)\n",
    "\n",
    "print(grid_search_gpr.best_params_)\n",
    "print(grid_search_gpr.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
