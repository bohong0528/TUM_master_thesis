{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-05T00:19:43.963037300Z",
     "start_time": "2024-01-05T00:19:43.928475600Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold, RandomizedSearchCV, StratifiedKFold\n",
    "from scipy.stats import expon, reciprocal, uniform\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, DotProduct, ExpSineSquared, RationalQuadratic\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFE, SelectFromModel, RFECV\n",
    "import mlxtend\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-05T00:08:59.821733Z",
     "start_time": "2024-01-05T00:08:33.233652700Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.loc[:, 'rush_hour'] = 0\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.drop(['end_time', 'max_dur', 'zoneId', 'cemdapStopDuration_s'], axis=1, inplace=True)\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.loc[:, 'rush_hour'] = 0\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.drop(['end_time', 'max_dur', 'zoneId', 'cemdapStopDuration_s'], axis=1, inplace=True)\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.loc[:, 'rush_hour'] = 0\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.drop(['end_time', 'max_dur', 'zoneId', 'cemdapStopDuration_s'], axis=1, inplace=True)\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.loc[:, 'rush_hour'] = 0\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.drop(['end_time', 'max_dur', 'zoneId', 'cemdapStopDuration_s'], axis=1, inplace=True)\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.loc[:, 'rush_hour'] = 0\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.drop(['end_time', 'max_dur', 'zoneId', 'cemdapStopDuration_s'], axis=1, inplace=True)\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.loc[:, 'rush_hour'] = 0\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.drop(['end_time', 'max_dur', 'zoneId', 'cemdapStopDuration_s'], axis=1, inplace=True)\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.loc[:, 'rush_hour'] = 0\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.drop(['end_time', 'max_dur', 'zoneId', 'cemdapStopDuration_s'], axis=1, inplace=True)\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.loc[:, 'rush_hour'] = 0\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.drop(['end_time', 'max_dur', 'zoneId', 'cemdapStopDuration_s'], axis=1, inplace=True)\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.loc[:, 'rush_hour'] = 0\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.drop(['end_time', 'max_dur', 'zoneId', 'cemdapStopDuration_s'], axis=1, inplace=True)\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.loc[:, 'rush_hour'] = 0\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.drop(['end_time', 'max_dur', 'zoneId', 'cemdapStopDuration_s'], axis=1, inplace=True)\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.loc[:, 'rush_hour'] = 0\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.drop(['end_time', 'max_dur', 'zoneId', 'cemdapStopDuration_s'], axis=1, inplace=True)\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.loc[:, 'rush_hour'] = 0\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.drop(['end_time', 'max_dur', 'zoneId', 'cemdapStopDuration_s'], axis=1, inplace=True)\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.loc[:, 'rush_hour'] = 0\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.drop(['end_time', 'max_dur', 'zoneId', 'cemdapStopDuration_s'], axis=1, inplace=True)\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.loc[:, 'rush_hour'] = 0\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.drop(['end_time', 'max_dur', 'zoneId', 'cemdapStopDuration_s'], axis=1, inplace=True)\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.loc[:, 'rush_hour'] = 0\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.drop(['end_time', 'max_dur', 'zoneId', 'cemdapStopDuration_s'], axis=1, inplace=True)\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.loc[:, 'rush_hour'] = 0\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.drop(['end_time', 'max_dur', 'zoneId', 'cemdapStopDuration_s'], axis=1, inplace=True)\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.loc[:, 'rush_hour'] = 0\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.drop(['end_time', 'max_dur', 'zoneId', 'cemdapStopDuration_s'], axis=1, inplace=True)\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.loc[:, 'rush_hour'] = 0\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.drop(['end_time', 'max_dur', 'zoneId', 'cemdapStopDuration_s'], axis=1, inplace=True)\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.loc[:, 'rush_hour'] = 0\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.drop(['end_time', 'max_dur', 'zoneId', 'cemdapStopDuration_s'], axis=1, inplace=True)\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.loc[:, 'rush_hour'] = 0\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_13824\\4058681877.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_rushhr.drop(['end_time', 'max_dur', 'zoneId', 'cemdapStopDuration_s'], axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "def load_data(file_list, df_activities, df_links_network):\n",
    "    data_frames = []\n",
    "    for file in file_list:\n",
    "        with open(file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            df_links = pd.DataFrame({\n",
    "                'link_id': data['links_id'],\n",
    "                'link_from': data['link_from'],\n",
    "                'link_to': data['link_to'],\n",
    "                'link_length': data['link_length'],\n",
    "                'link_freespeed': data['link_freespeed'],\n",
    "                'link_capacity': data['link_capacity'],\n",
    "                'link_permlanes': data['link_permlanes'],\n",
    "                'link_counts': data['link_counts']\n",
    "            })\n",
    "            df_nodes = pd.DataFrame({\n",
    "                'node_id': data['nodes_id'],\n",
    "                'node_x': data['nodes_x'],\n",
    "                'node_y': data['nodes_y']\n",
    "            })\n",
    "            df_od_pairs = pd.DataFrame(data['o_d_pairs'], columns=['origin', 'destination'])\n",
    "            \n",
    "            df_work = pd.DataFrame({\n",
    "                        'work_x': data['work_x'],\n",
    "                        'work_y': data['work_y'],\n",
    "                        'go_to_work': data['go_to_work']\n",
    "            })\n",
    "            df_home = pd.DataFrame({\n",
    "                'home_x': data['home_x'],\n",
    "                'home_y': data['home_y'],\n",
    "                'go_to_home': data['go_to_home']\n",
    "            })\n",
    "            \n",
    "            df_links = df_links.merge(df_nodes, how='left', left_on='link_from', right_on='node_id')\n",
    "            df_links = df_links.rename(columns={'node_x': 'start_node_x', 'node_y': 'start_node_y'})\n",
    "            df_links.drop('node_id', axis=1, inplace=True)\n",
    "            df_links = df_links.merge(df_nodes, how='left', left_on='link_to', right_on='node_id')\n",
    "            df_links = df_links.rename(columns={'node_x': 'end_node_x', 'node_y': 'end_node_y'})\n",
    "            df_links.drop('node_id', axis=1, inplace=True) \n",
    "            \n",
    "            origin_counts = df_od_pairs['origin'].value_counts()\n",
    "            df_origin_counts = origin_counts.reset_index()\n",
    "            df_origin_counts.columns = ['origin', 'start_count']\n",
    "            destination_counts = df_od_pairs['destination'].value_counts()\n",
    "            df_destination_counts = destination_counts.reset_index()\n",
    "            df_destination_counts.columns = ['destination', 'end_count']\n",
    "            df_links = df_links.merge(df_origin_counts, how='left', left_on='link_from', right_on='origin')\n",
    "            df_links.drop('origin', axis=1, inplace=True)\n",
    "            df_links = df_links.merge(df_destination_counts, how='left', left_on='link_to', right_on='destination')\n",
    "            df_links.drop('destination', axis=1, inplace=True)\n",
    "            df_links[['start_count','end_count']] = df_links[['start_count','end_count']].fillna(-1)\n",
    "            \n",
    "            # Calculate time of go_to_work and go_to_sum\n",
    "            df_act_work = df_activities[df_activities['activity_type_main']=='work'].drop(['end_time'], axis=1)\n",
    "            df_act_work = df_act_work.merge(df_work, how='left', left_on=['x','y'], right_on=['work_x','work_y'])\n",
    "            df_act_work.drop(['x','y'], axis=1, inplace=True)\n",
    "            df_act_work_agg = df_act_work.groupby(by=\"link\")['go_to_work'].sum().reset_index(drop=False)\n",
    "            df_act_home = df_activities[df_activities['activity_type_main']=='home'].drop(['end_time'], axis=1)\n",
    "            df_act_home = df_act_home.merge(df_home, how='left', left_on=['x','y'], right_on=['home_x','home_y'])\n",
    "            df_act_home.drop(['x','y'], axis=1, inplace=True)\n",
    "            df_act_home_agg = df_act_home.groupby(by=\"link\")['go_to_home'].sum().reset_index(drop=False)\n",
    "            df_act_agg = df_act_home_agg.merge(df_act_work_agg, how='outer', on='link')\n",
    "            df_act_agg.fillna(0, inplace=True)\n",
    "            df_act_agg['go_to_sum'] = df_act_agg['go_to_home'] + df_act_agg['go_to_work']\n",
    "            \n",
    "            df_rushhr = df_activities[df_activities['end_time']!=-1]\n",
    "            df_rushhr.loc[:, 'rush_hour'] = 0\n",
    "            df_rushhr.loc[df_rushhr['end_time'].between(pd.to_timedelta('08:00:00'), pd.to_timedelta('10:00:00'), inclusive='both'), 'rush_hour'] = 1\n",
    "            df_rushhr.loc[df_rushhr['end_time'].between(pd.to_timedelta('16:00:00'), pd.to_timedelta('19:00:00'), inclusive='both'), 'rush_hour'] = 1\n",
    "            df_rushhr.drop(['end_time', 'max_dur', 'zoneId', 'cemdapStopDuration_s'], axis=1, inplace=True)\n",
    "            df_rushhragg = df_rushhr.groupby(by=\"link\").sum()['rush_hour'].reset_index(drop=False)\n",
    "            \n",
    "            df_maxduragg = df_activities[df_activities['max_dur']!=-1].groupby(by='link')['max_dur'].sum().reset_index(drop=False)\n",
    "            \n",
    "            df_activities['cemdapStopDuration_s'] = df_activities['cemdapStopDuration_s'].astype(float)\n",
    "            df_cemagg = df_activities[df_activities['cemdapStopDuration_s']!=-1].groupby(by='link')['cemdapStopDuration_s'].sum().reset_index(drop=False)\n",
    "            \n",
    "            df_temp = df_links.merge(df_links_network, how='left', on=['start_node_x','start_node_y','end_node_x','end_node_y'])\n",
    "            df_temp = df_temp[['link_id_x','link_from','link_to','link_id_y','from', 'to', 'type']]\n",
    "            df_temp = df_temp.merge(df_act_agg, how='left', left_on='link_id_y', right_on='link')\n",
    "            df_temp.drop('link', axis=1, inplace=True)\n",
    "            df_temp = df_temp.merge(df_rushhragg, how='left', left_on='link_id_y', right_on='link')\n",
    "            df_temp.drop('link', axis=1, inplace=True)\n",
    "            df_temp = df_temp.merge(df_maxduragg, how='left', left_on='link_id_y', right_on='link')\n",
    "            df_temp.drop('link', axis=1, inplace=True)\n",
    "            df_temp = df_temp.merge(df_cemagg, how='left', left_on='link_id_y', right_on='link')\n",
    "            df_temp.fillna({'cemdapStopDuration_s':-1, 'max_dur':-1, 'rush_hour': -1, 'go_to_sum': -1}, inplace=True)\n",
    "            df_temp = df_temp[['link_id_x', 'go_to_sum', 'rush_hour', 'max_dur', 'cemdapStopDuration_s', 'type']]\n",
    "            \n",
    "            df_links = df_links.merge(df_temp, how='left', left_on='link_id', right_on='link_id_x')\n",
    "            df_links.drop('link_id_x', axis=1, inplace=True)\n",
    "        data_frames.append(df_links)\n",
    "    return pd.concat(data_frames, ignore_index=True)\n",
    "# \n",
    "# \n",
    "train_files = ['s-0.json', 's-1.json', 's-2.json', 's-3.json', 's-4.json','s-5.json', 's-6.json', 's-7.json', 's-8.json', 's-9.json'] \n",
    "test_files = ['s-15.json', 's-16.json', 's-17.json', 's-18.json','s-19.json']\n",
    "validate_files = ['s-10.json', 's-11.json', 's-12.json', 's-13.json','s-14.json']\n",
    "train_files = ['Data/cutoutWorlds/Train/po-1_pn-1.0_sn-1/' + i for i in train_files]\n",
    "test_files = ['Data/cutoutWorlds/Test/po-1_pn-1.0_sn-1/' + j for j in test_files]\n",
    "validate_files = ['Data/cutoutWorlds/Validate/po-1_pn-1.0_sn-1/' + k for k in validate_files]\n",
    "df_activities = pd.read_pickle(\"Data/cutoutWorlds/Train/po-1_pn-1.0_sn-1/df_activities.pkl\")\n",
    "df_links_network = pd.read_pickle(\"Data/cutoutWorlds/Train/po-1_pn-1.0_sn-1/df_links_network.pkl\")\n",
    "train_data = load_data(train_files, df_activities, df_links_network)\n",
    "test_data = load_data(test_files, df_activities, df_links_network)\n",
    "validate_data = load_data(validate_files, df_activities, df_links_network)\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "       link_id  link_from  link_to  link_length  link_freespeed  \\\n0            0        425      579   134.962910        4.166667   \n1            1        579      425   134.962910        4.166667   \n2            2        524      620    49.508163        6.944444   \n3            3        620      524    49.508163        6.944444   \n4            4        656      652    13.326026        6.944444   \n...        ...        ...      ...          ...             ...   \n16980     1391        212      704  1124.319428       22.222222   \n16981     1392        678      679    43.946365       22.222222   \n16982     1393        679      678    43.946365       22.222222   \n16983     1394        679      412    42.684073       22.222222   \n16984     1395        412      679    42.684073       22.222222   \n\n       link_capacity  link_permlanes  link_counts  start_node_x  start_node_y  \\\n0              300.0             0.5          0.0  4.609957e+06  5.819853e+06   \n1              300.0             0.5          0.0  4.609956e+06  5.819988e+06   \n2             1800.0             1.5         27.0  4.614751e+06  5.819976e+06   \n3             2400.0             2.0         35.0  4.614750e+06  5.820025e+06   \n4             1600.0             1.0         79.0  4.615677e+06  5.819981e+06   \n...              ...             ...          ...           ...           ...   \n16980          600.0             1.0         10.0  4.624410e+06  5.875182e+06   \n16981         6000.0             1.5         36.0  4.643553e+06  5.884511e+06   \n16982         6000.0             1.5         33.0  4.643596e+06  5.884518e+06   \n16983         6000.0             1.5         35.0  4.643596e+06  5.884518e+06   \n16984         6000.0             1.5         32.0  4.643639e+06  5.884524e+06   \n\n         end_node_x    end_node_y  start_count  end_count  go_to_sum  \\\n0      4.609956e+06  5.819988e+06         -1.0       -1.0       -1.0   \n1      4.609957e+06  5.819853e+06         -1.0       -1.0       -1.0   \n2      4.614750e+06  5.820025e+06         -1.0        2.0       -1.0   \n3      4.614751e+06  5.819976e+06          2.0       -1.0       -1.0   \n4      4.615681e+06  5.819993e+06          1.0        9.0       -1.0   \n...             ...           ...          ...        ...        ...   \n16980  4.623684e+06  5.874330e+06         -1.0       -1.0       -1.0   \n16981  4.643596e+06  5.884518e+06         13.0        2.0       -1.0   \n16982  4.643553e+06  5.884511e+06          2.0       13.0       -1.0   \n16983  4.643639e+06  5.884524e+06          2.0       -1.0       -1.0   \n16984  4.643596e+06  5.884518e+06         -1.0        2.0       -1.0   \n\n       rush_hour  max_dur  cemdapStopDuration_s         type  \n0           -1.0     -1.0                  -1.0  residential  \n1           -1.0     -1.0                  -1.0  residential  \n2           -1.0     -1.0                  -1.0     tertiary  \n3           -1.0     -1.0                  -1.0     tertiary  \n4           -1.0     -1.0                  -1.0    secondary  \n...          ...      ...                   ...          ...  \n16980       -1.0     -1.0                  -1.0     tertiary  \n16981        0.0    688.0               21000.0        trunk  \n16982       -1.0     -1.0                  -1.0        trunk  \n16983       -1.0   1380.0                1380.0        trunk  \n16984       -1.0     -1.0                  -1.0        trunk  \n\n[16985 rows x 19 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>link_id</th>\n      <th>link_from</th>\n      <th>link_to</th>\n      <th>link_length</th>\n      <th>link_freespeed</th>\n      <th>link_capacity</th>\n      <th>link_permlanes</th>\n      <th>link_counts</th>\n      <th>start_node_x</th>\n      <th>start_node_y</th>\n      <th>end_node_x</th>\n      <th>end_node_y</th>\n      <th>start_count</th>\n      <th>end_count</th>\n      <th>go_to_sum</th>\n      <th>rush_hour</th>\n      <th>max_dur</th>\n      <th>cemdapStopDuration_s</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>425</td>\n      <td>579</td>\n      <td>134.962910</td>\n      <td>4.166667</td>\n      <td>300.0</td>\n      <td>0.5</td>\n      <td>0.0</td>\n      <td>4.609957e+06</td>\n      <td>5.819853e+06</td>\n      <td>4.609956e+06</td>\n      <td>5.819988e+06</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>residential</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>579</td>\n      <td>425</td>\n      <td>134.962910</td>\n      <td>4.166667</td>\n      <td>300.0</td>\n      <td>0.5</td>\n      <td>0.0</td>\n      <td>4.609956e+06</td>\n      <td>5.819988e+06</td>\n      <td>4.609957e+06</td>\n      <td>5.819853e+06</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>residential</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>524</td>\n      <td>620</td>\n      <td>49.508163</td>\n      <td>6.944444</td>\n      <td>1800.0</td>\n      <td>1.5</td>\n      <td>27.0</td>\n      <td>4.614751e+06</td>\n      <td>5.819976e+06</td>\n      <td>4.614750e+06</td>\n      <td>5.820025e+06</td>\n      <td>-1.0</td>\n      <td>2.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>tertiary</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>620</td>\n      <td>524</td>\n      <td>49.508163</td>\n      <td>6.944444</td>\n      <td>2400.0</td>\n      <td>2.0</td>\n      <td>35.0</td>\n      <td>4.614750e+06</td>\n      <td>5.820025e+06</td>\n      <td>4.614751e+06</td>\n      <td>5.819976e+06</td>\n      <td>2.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>tertiary</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>656</td>\n      <td>652</td>\n      <td>13.326026</td>\n      <td>6.944444</td>\n      <td>1600.0</td>\n      <td>1.0</td>\n      <td>79.0</td>\n      <td>4.615677e+06</td>\n      <td>5.819981e+06</td>\n      <td>4.615681e+06</td>\n      <td>5.819993e+06</td>\n      <td>1.0</td>\n      <td>9.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>secondary</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>16980</th>\n      <td>1391</td>\n      <td>212</td>\n      <td>704</td>\n      <td>1124.319428</td>\n      <td>22.222222</td>\n      <td>600.0</td>\n      <td>1.0</td>\n      <td>10.0</td>\n      <td>4.624410e+06</td>\n      <td>5.875182e+06</td>\n      <td>4.623684e+06</td>\n      <td>5.874330e+06</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>tertiary</td>\n    </tr>\n    <tr>\n      <th>16981</th>\n      <td>1392</td>\n      <td>678</td>\n      <td>679</td>\n      <td>43.946365</td>\n      <td>22.222222</td>\n      <td>6000.0</td>\n      <td>1.5</td>\n      <td>36.0</td>\n      <td>4.643553e+06</td>\n      <td>5.884511e+06</td>\n      <td>4.643596e+06</td>\n      <td>5.884518e+06</td>\n      <td>13.0</td>\n      <td>2.0</td>\n      <td>-1.0</td>\n      <td>0.0</td>\n      <td>688.0</td>\n      <td>21000.0</td>\n      <td>trunk</td>\n    </tr>\n    <tr>\n      <th>16982</th>\n      <td>1393</td>\n      <td>679</td>\n      <td>678</td>\n      <td>43.946365</td>\n      <td>22.222222</td>\n      <td>6000.0</td>\n      <td>1.5</td>\n      <td>33.0</td>\n      <td>4.643596e+06</td>\n      <td>5.884518e+06</td>\n      <td>4.643553e+06</td>\n      <td>5.884511e+06</td>\n      <td>2.0</td>\n      <td>13.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>trunk</td>\n    </tr>\n    <tr>\n      <th>16983</th>\n      <td>1394</td>\n      <td>679</td>\n      <td>412</td>\n      <td>42.684073</td>\n      <td>22.222222</td>\n      <td>6000.0</td>\n      <td>1.5</td>\n      <td>35.0</td>\n      <td>4.643596e+06</td>\n      <td>5.884518e+06</td>\n      <td>4.643639e+06</td>\n      <td>5.884524e+06</td>\n      <td>2.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>1380.0</td>\n      <td>1380.0</td>\n      <td>trunk</td>\n    </tr>\n    <tr>\n      <th>16984</th>\n      <td>1395</td>\n      <td>412</td>\n      <td>679</td>\n      <td>42.684073</td>\n      <td>22.222222</td>\n      <td>6000.0</td>\n      <td>1.5</td>\n      <td>32.0</td>\n      <td>4.643639e+06</td>\n      <td>5.884524e+06</td>\n      <td>4.643596e+06</td>\n      <td>5.884518e+06</td>\n      <td>-1.0</td>\n      <td>2.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>-1.0</td>\n      <td>trunk</td>\n    </tr>\n  </tbody>\n</table>\n<p>16985 rows Ã— 19 columns</p>\n</div>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T00:09:24.926518300Z",
     "start_time": "2024-01-05T00:09:24.853461400Z"
    }
   },
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T20:01:05.026429300Z",
     "start_time": "2023-12-29T20:01:04.818365100Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Initialize a list to hold trips\n",
    "# trips = []\n",
    "# current_trip = [df_od_pairs.iloc[0]['origin']]  # Start with the first origin\n",
    "# \n",
    "# # Iterate over the DataFrame rows\n",
    "# for i, row in df_od_pairs.iterrows():\n",
    "#     current_trip.append(row['destination'])  # Always add the destination\n",
    "#     # Check if the next origin matches the current destination\n",
    "#     if i + 1 < len(df_od_pairs) and row['destination'] != df_od_pairs.iloc[i + 1]['origin']:\n",
    "#         # If it doesn't, the current trip has ended\n",
    "#         trips.append(current_trip)\n",
    "#         current_trip = [df_od_pairs.iloc[i + 1]['origin']]  # Start a new trip\n",
    "# \n",
    "# # Add the last trip if it wasn't already added\n",
    "# if current_trip not in trips:\n",
    "#     trips.append(current_trip)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T20:01:05.028445Z",
     "start_time": "2023-12-29T20:01:04.975096700Z"
    }
   },
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# # Flatten the list of trips into a single list of nodes including origins and destinations\n",
    "# all_nodes = [node for trip in trips for node in trip]\n",
    "# \n",
    "# # Use Counter to count the occurrences of each node\n",
    "# node_trip_counts = Counter(all_nodes)\n",
    "# \n",
    "# df_node_trip_counts = pd.DataFrame.from_dict(node_trip_counts, orient='index').reset_index()\n",
    "# df_node_trip_counts.columns = ['node_id', 'trip_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-05T00:25:31.053189200Z",
     "start_time": "2024-01-05T00:25:30.986858300Z"
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "numerical_features = ['link_length', 'link_freespeed', 'link_capacity', 'link_permlanes', 'start_node_x', 'start_node_y', 'end_node_x', 'end_node_y', 'start_count', 'end_count', 'go_to_sum', 'rush_hour', 'max_dur', 'cemdapStopDuration_s']\n",
    "category_feature = 'type'\n",
    "X_t = train_data.drop(columns=['link_counts'])\n",
    "y_t = train_data['link_counts']\n",
    "X_v = validate_data.drop(columns=['link_counts'])\n",
    "y_v = validate_data['link_counts']\n",
    "X_te = test_data.drop(columns=['link_counts'])\n",
    "y_te = test_data['link_counts']\n",
    "scaler = StandardScaler()\n",
    "le = LabelEncoder()\n",
    "ohe = OneHotEncoder()\n",
    "X_t[numerical_features] = scaler.fit_transform(X_t[numerical_features])\n",
    "X_v[numerical_features] = scaler.fit_transform(X_v[numerical_features])\n",
    "X_te[numerical_features] = scaler.fit_transform(X_te[numerical_features])\n",
    "X_t[category_feature] = le.fit_transform(X_t[category_feature])\n",
    "X_v[category_feature] = le.fit_transform(X_v[category_feature])\n",
    "X_te[category_feature] = le.fit_transform(X_te[category_feature])\n",
    "# X_t = scaler.fit_transform(X_t)\n",
    "# X_v = scaler.fit_transform(X_v)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "       link_id  link_from  link_to  link_length  link_freespeed  \\\n0            0        425      579    -0.104697       -0.438269   \n1            1        579      425    -0.104697       -0.438269   \n2            2        524      620    -0.407803        0.125463   \n3            3        620      524    -0.407803        0.125463   \n4            4        656      652    -0.536141        0.125463   \n...        ...        ...      ...          ...             ...   \n16980     1391        212      704     3.404536        3.225985   \n16981     1392        678      679    -0.427531        3.225985   \n16982     1393        679      678    -0.427531        3.225985   \n16983     1394        679      412    -0.432008        3.225985   \n16984     1395        412      679    -0.432008        3.225985   \n\n       link_capacity  link_permlanes  start_node_x  start_node_y  end_node_x  \\\n0          -0.713008       -1.331215      0.585691     -0.140243    0.585637   \n1          -0.713008       -1.331215      0.585641     -0.133567    0.585686   \n2           0.425398        0.690756      0.905173     -0.134153    0.905136   \n3           0.880761        1.701741      0.905138     -0.131705    0.905171   \n4           0.273611       -0.320230      0.966912     -0.133921    0.967171   \n...              ...             ...           ...           ...         ...   \n16980      -0.485327       -0.320230      1.548854      2.596430    1.500500   \n16981       3.612936        0.690756      2.824558      3.057890    2.827464   \n16982       3.612936        0.690756      2.827453      3.058218    2.824569   \n16983       3.612936        0.690756      2.827453      3.058218    2.830281   \n16984       3.612936        0.690756      2.830270      3.058511    2.827464   \n\n       end_node_y  start_count  end_count  go_to_sum  rush_hour   max_dur  \\\n0       -0.133553    -0.225273  -0.205094  -0.245133  -0.389305 -0.295188   \n1       -0.140229    -0.225273  -0.205094  -0.245133  -0.389305 -0.295188   \n2       -0.131691    -0.225273   0.009828  -0.245133  -0.389305 -0.295188   \n3       -0.134140     0.019634  -0.205094  -0.245133  -0.389305 -0.295188   \n4       -0.133277    -0.062002   0.511311  -0.245133  -0.389305 -0.295188   \n...           ...          ...        ...        ...        ...       ...   \n16980    2.554400    -0.225273  -0.205094  -0.245133  -0.389305 -0.295188   \n16981    3.058326     0.917627   0.009828  -0.245133   0.901664 -0.018193   \n16982    3.057998     0.019634   0.797872  -0.245133  -0.389305 -0.295188   \n16983    3.058618     0.019634  -0.205094  -0.245133  -0.389305  0.260008   \n16984    3.058326    -0.225273   0.009828  -0.245133  -0.389305 -0.295188   \n\n       cemdapStopDuration_s  type  \n0                 -0.305904     5  \n1                 -0.305904     5  \n2                 -0.305904     8  \n3                 -0.305904     8  \n4                 -0.305904     6  \n...                     ...   ...  \n16980             -0.305904     8  \n16981              0.243322     9  \n16982             -0.305904     9  \n16983             -0.269787     9  \n16984             -0.305904     9  \n\n[16985 rows x 18 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>link_id</th>\n      <th>link_from</th>\n      <th>link_to</th>\n      <th>link_length</th>\n      <th>link_freespeed</th>\n      <th>link_capacity</th>\n      <th>link_permlanes</th>\n      <th>start_node_x</th>\n      <th>start_node_y</th>\n      <th>end_node_x</th>\n      <th>end_node_y</th>\n      <th>start_count</th>\n      <th>end_count</th>\n      <th>go_to_sum</th>\n      <th>rush_hour</th>\n      <th>max_dur</th>\n      <th>cemdapStopDuration_s</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>425</td>\n      <td>579</td>\n      <td>-0.104697</td>\n      <td>-0.438269</td>\n      <td>-0.713008</td>\n      <td>-1.331215</td>\n      <td>0.585691</td>\n      <td>-0.140243</td>\n      <td>0.585637</td>\n      <td>-0.133553</td>\n      <td>-0.225273</td>\n      <td>-0.205094</td>\n      <td>-0.245133</td>\n      <td>-0.389305</td>\n      <td>-0.295188</td>\n      <td>-0.305904</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>579</td>\n      <td>425</td>\n      <td>-0.104697</td>\n      <td>-0.438269</td>\n      <td>-0.713008</td>\n      <td>-1.331215</td>\n      <td>0.585641</td>\n      <td>-0.133567</td>\n      <td>0.585686</td>\n      <td>-0.140229</td>\n      <td>-0.225273</td>\n      <td>-0.205094</td>\n      <td>-0.245133</td>\n      <td>-0.389305</td>\n      <td>-0.295188</td>\n      <td>-0.305904</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>524</td>\n      <td>620</td>\n      <td>-0.407803</td>\n      <td>0.125463</td>\n      <td>0.425398</td>\n      <td>0.690756</td>\n      <td>0.905173</td>\n      <td>-0.134153</td>\n      <td>0.905136</td>\n      <td>-0.131691</td>\n      <td>-0.225273</td>\n      <td>0.009828</td>\n      <td>-0.245133</td>\n      <td>-0.389305</td>\n      <td>-0.295188</td>\n      <td>-0.305904</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>620</td>\n      <td>524</td>\n      <td>-0.407803</td>\n      <td>0.125463</td>\n      <td>0.880761</td>\n      <td>1.701741</td>\n      <td>0.905138</td>\n      <td>-0.131705</td>\n      <td>0.905171</td>\n      <td>-0.134140</td>\n      <td>0.019634</td>\n      <td>-0.205094</td>\n      <td>-0.245133</td>\n      <td>-0.389305</td>\n      <td>-0.295188</td>\n      <td>-0.305904</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>656</td>\n      <td>652</td>\n      <td>-0.536141</td>\n      <td>0.125463</td>\n      <td>0.273611</td>\n      <td>-0.320230</td>\n      <td>0.966912</td>\n      <td>-0.133921</td>\n      <td>0.967171</td>\n      <td>-0.133277</td>\n      <td>-0.062002</td>\n      <td>0.511311</td>\n      <td>-0.245133</td>\n      <td>-0.389305</td>\n      <td>-0.295188</td>\n      <td>-0.305904</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>16980</th>\n      <td>1391</td>\n      <td>212</td>\n      <td>704</td>\n      <td>3.404536</td>\n      <td>3.225985</td>\n      <td>-0.485327</td>\n      <td>-0.320230</td>\n      <td>1.548854</td>\n      <td>2.596430</td>\n      <td>1.500500</td>\n      <td>2.554400</td>\n      <td>-0.225273</td>\n      <td>-0.205094</td>\n      <td>-0.245133</td>\n      <td>-0.389305</td>\n      <td>-0.295188</td>\n      <td>-0.305904</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>16981</th>\n      <td>1392</td>\n      <td>678</td>\n      <td>679</td>\n      <td>-0.427531</td>\n      <td>3.225985</td>\n      <td>3.612936</td>\n      <td>0.690756</td>\n      <td>2.824558</td>\n      <td>3.057890</td>\n      <td>2.827464</td>\n      <td>3.058326</td>\n      <td>0.917627</td>\n      <td>0.009828</td>\n      <td>-0.245133</td>\n      <td>0.901664</td>\n      <td>-0.018193</td>\n      <td>0.243322</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>16982</th>\n      <td>1393</td>\n      <td>679</td>\n      <td>678</td>\n      <td>-0.427531</td>\n      <td>3.225985</td>\n      <td>3.612936</td>\n      <td>0.690756</td>\n      <td>2.827453</td>\n      <td>3.058218</td>\n      <td>2.824569</td>\n      <td>3.057998</td>\n      <td>0.019634</td>\n      <td>0.797872</td>\n      <td>-0.245133</td>\n      <td>-0.389305</td>\n      <td>-0.295188</td>\n      <td>-0.305904</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>16983</th>\n      <td>1394</td>\n      <td>679</td>\n      <td>412</td>\n      <td>-0.432008</td>\n      <td>3.225985</td>\n      <td>3.612936</td>\n      <td>0.690756</td>\n      <td>2.827453</td>\n      <td>3.058218</td>\n      <td>2.830281</td>\n      <td>3.058618</td>\n      <td>0.019634</td>\n      <td>-0.205094</td>\n      <td>-0.245133</td>\n      <td>-0.389305</td>\n      <td>0.260008</td>\n      <td>-0.269787</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>16984</th>\n      <td>1395</td>\n      <td>412</td>\n      <td>679</td>\n      <td>-0.432008</td>\n      <td>3.225985</td>\n      <td>3.612936</td>\n      <td>0.690756</td>\n      <td>2.830270</td>\n      <td>3.058511</td>\n      <td>2.827464</td>\n      <td>3.058326</td>\n      <td>-0.225273</td>\n      <td>0.009828</td>\n      <td>-0.245133</td>\n      <td>-0.389305</td>\n      <td>-0.295188</td>\n      <td>-0.305904</td>\n      <td>9</td>\n    </tr>\n  </tbody>\n</table>\n<p>16985 rows Ã— 18 columns</p>\n</div>"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-05T00:25:38.793260300Z",
     "start_time": "2024-01-05T00:25:38.746926800Z"
    }
   },
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    # 'Linear Regression': LinearRegression(),\n",
    "    # 'Lasso': LassoCV(cv=3, random_state=42, max_iter=100000),\n",
    "    # 'Ridge': RidgeCV(cv=3),\n",
    "    'Random Forest': RandomForestRegressor(criterion='friedman_mse', max_depth=20, min_samples_leaf=2, n_estimators=150, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(max_depth=5, min_samples_split=5, random_state=42, subsample=0.8),\n",
    "    'Artificial Neural Network': MLPRegressor(activation='tanh', alpha=0.001, learning_rate_init=0.01, max_iter=1000, random_state=42, solver='sgd'),\n",
    "    # 'SVR': SVR(C=25.383309585489613, epsilon=0.01, gamma=2.1969677491639246, max_iter=3000),\n",
    "#     'Gaussian Process Regression': GaussianProcessRegressor(kernel=RBF(length_scale=1.0) + WhiteKernel(noise_level=1.0), alpha=0.1, n_restarts_optimizer=3)\n",
    "}\n",
    "\n",
    "# Function to train and evaluate models\n",
    "def feature_select_models(models, X_train, y_train, X_test, y_test):\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        if name not in ['SVR', 'Artificial Neural Network', 'Gaussian Process Regression']:\n",
    "            selector = EFS(model, min_features=6, max_features=12, print_progress=True, scoring='neg_mean_squared_error', cv=0, n_jobs=-1).fit(X_train, y_train)\n",
    "        else:\n",
    "            # Fit Random Forest to get feature importances\n",
    "            rf = RandomForestRegressor()\n",
    "            rf.fit(X_train, y_train)\n",
    "            selector = EFS(rf, min_features=6, max_features=12, print_progress=True, scoring='neg_mean_squared_error', cv=0, n_jobs=-1).fit(X_train, y_train)\n",
    "        selected_features = selector.best_feature_names_\n",
    "        X_train_reduced = X_train[list(selected_features)]\n",
    "        X_test_reduced = X_test[list(selected_features)]\n",
    "        model.fit(X_train_reduced, y_train)\n",
    "        y_pred = model.predict(X_test_reduced)\n",
    "       \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        print(f\"{name}: MSE: {mse}, MAE: {mae}\")\n",
    "\n",
    "        results[name] = {'MAE': mae, 'MSE': mse, 'selected_feature': selected_features}\n",
    "\n",
    "            \n",
    "    return results\n",
    "\n",
    "# Train and evaluate\n",
    "# results = evaluate_models(models, X_t, y_t, X_v, y_v)\n",
    "\n",
    "# # Display Results\n",
    "# for model_name, metrics in results.items():\n",
    "#     print(\"\\n\")\n",
    "#     print(f\"Model: {model_name}\")\n",
    "#     for metric_name, value in metrics.items():\n",
    "#         print(f\"{metric_name}: {value}\")\n",
    "#     print(\"\\n\")\n",
    "\n",
    "results_feature = feature_select_models(models, X_t, y_t, X_te, y_te)\n",
    "# for model_name, metrics in results_feature.items():\n",
    "#     print(\"\\n\")\n",
    "#     print(f\"Model: {model_name}\")\n",
    "#     for metric_name, value in metrics.items():\n",
    "#         print(f\"{metric_name}: {value}\")\n",
    "#     print(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-04T16:55:08.161116600Z",
     "start_time": "2024-01-04T16:38:09.099829700Z"
    }
   },
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-05T00:31:29.592973900Z",
     "start_time": "2024-01-05T00:28:22.015669500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression done\n",
      "Lasso done\n",
      "Ridge done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LinBoH\\Desktop\\TUM_master_thesis\\venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR done\n",
      "Random Forest done\n",
      "Gradient Boosting done\n",
      "Artificial Neural Network done\n",
      "\n",
      "\n",
      "Model: Linear Regression\n",
      "MAE: 18.981209515118735\n",
      "MSE: 1337.4049246155691\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: Lasso\n",
      "MAE: 17.345601766237277\n",
      "MSE: 1214.5567030480158\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: Ridge\n",
      "MAE: 18.970325094645194\n",
      "MSE: 1336.6470349011954\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: SVR\n",
      "MAE: 23.79457835083277\n",
      "MSE: 1756.0961180857178\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: Random Forest\n",
      "MAE: 17.1703006966335\n",
      "MSE: 1833.07105527877\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: Gradient Boosting\n",
      "MAE: 15.269573563443997\n",
      "MSE: 1550.6097362929238\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: Artificial Neural Network\n",
      "MAE: 22.649647892871418\n",
      "MSE: 1779.2315184067284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Lasso': LassoCV(cv=3, random_state=42, max_iter=100000),\n",
    "    'Ridge': RidgeCV(cv=3),\n",
    "    'SVR': SVR(C=25.383309585489613, epsilon=0.01, gamma=2.1969677491639246, max_iter=3000),\n",
    "    'Random Forest': RandomForestRegressor(criterion='friedman_mse', max_depth=20, min_samples_leaf=2, n_estimators=150, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(max_depth=5, min_samples_split=5, random_state=42, subsample=0.8),\n",
    "    'Artificial Neural Network': MLPRegressor(activation='tanh', alpha=0.001, learning_rate_init=0.01, max_iter=1000, random_state=42, solver='sgd'),\n",
    "#     'Gaussian Process Regression': GaussianProcessRegressor(kernel=RBF(length_scale=1.0) + WhiteKernel(noise_level=1.0), alpha=0.1, n_restarts_optimizer=3)\n",
    "}\n",
    "\n",
    "# Function to train and evaluate models\n",
    "def evaluate_models(models, X_train, y_train, X_test, y_test):\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "#         kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "#         cv_scores = cross_val_score(model, X_train, y_train, cv=kf, scoring='neg_mean_squared_error')\n",
    "#         mse_scores = -cv_scores \n",
    "#         mean_mse = mse_scores.mean()\n",
    "#         std_mse = mse_scores.std()\n",
    "        # mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "        # r2 = r2_score(y_test, y_pred)\n",
    "        print(name + \" done\")\n",
    "        \n",
    "        results[name] = {'MAE': mae, 'MSE': mse}\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to train and evaluate models\n",
    "def feature_select_models(models, X_train, y_train, X_test, y_test):\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        if name not in ['SVR', 'Artificial Neural Network', 'Gaussian Process Regression']:\n",
    "            selector = RFECV(model, step=1, cv=StratifiedKFold(5), scoring='neg_mean_squared_error').fit(X_train, y_train)\n",
    "            # To see which features are selected\n",
    "            selected_features = X_train.columns[selector.support_]\n",
    "            y_pred = selector.predict(X_test)\n",
    "        else:\n",
    "            # Fit Random Forest to get feature importances\n",
    "            rf = RandomForestRegressor()\n",
    "            rf.fit(X_train, y_train)\n",
    "\n",
    "            # Select features based on importances\n",
    "            rfe = RFECV(estimator=rf, step=1, cv=StratifiedKFold(5), scoring='neg_mean_squared_error').fit(X_train, y_train)\n",
    "            selected_features = X_train.columns[rfe.support_]\n",
    "            X_train_reduced = X_train[selected_features]\n",
    "            X_test_reduced = X_test[selected_features]\n",
    "            model.fit(X_train_reduced, y_train)\n",
    "            y_pred = model.predict(X_test_reduced)\n",
    "       \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        print(name + \" done\")\n",
    "\n",
    "        results[name] = {'MAE': mae, 'MSE': mse, 'selected_feature': selected_features}\n",
    "\n",
    "            \n",
    "    return results\n",
    "\n",
    "# Train and evaluate\n",
    "results = evaluate_models(models, X_t, y_t, X_v, y_v)\n",
    "\n",
    "# Display Results\n",
    "for model_name, metrics in results.items():\n",
    "    print(\"\\n\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"{metric_name}: {value}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# results_feature = feature_select_models(models, X_t, y_t, X_te, y_te)\n",
    "# for model_name, metrics in results_feature.items():\n",
    "#     print(\"\\n\")\n",
    "#     print(f\"Model: {model_name}\")\n",
    "#     for metric_name, value in metrics.items():\n",
    "#         print(f\"{metric_name}: {value}\")\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('result_after_featureselection.pkl', 'wb') as file:\n",
    "    pickle.dump(results_feature, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Linear Regression': {'MAE': 19.000541966353587, 'MSE': 1350.56069460534},\n",
       " 'Lasso': {'MAE': 22.610703312854127, 'MSE': 1679.4760262080995},\n",
       " 'Ridge': {'MAE': 18.989592790164192, 'MSE': 1349.7480663335148},\n",
       " 'SVR': {'MAE': 23.793351948629688, 'MSE': 1756.1078902638878},\n",
       " 'Random Forest': {'MAE': 17.635830260141653, 'MSE': 2015.2344103327919},\n",
       " 'Gradient Boosting': {'MAE': 15.969274342418169, 'MSE': 1596.9989870917648},\n",
       " 'Artificial Neural Network': {'MAE': 21.345942437519778,\n",
       "  'MSE': 1888.5103715269568}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Linear Regression': {'MAE': 19.47148762371669,\n",
       "  'MSE': 874.7590684153923,\n",
       "  'selected_feature': Index(['link_length', 'link_freespeed', 'link_capacity', 'link_permlanes',\n",
       "         'start_node_x', 'start_node_y', 'end_node_x', 'end_node_y',\n",
       "         'start_count'],\n",
       "        dtype='object')},\n",
       " 'Lasso': {'MAE': 21.29080224423866,\n",
       "  'MSE': 995.8290100956449,\n",
       "  'selected_feature': Index(['link_id', 'link_permlanes', 'start_node_x', 'start_node_y',\n",
       "         'end_node_x', 'end_node_y', 'start_count', 'end_count', 'go_to_sum'],\n",
       "        dtype='object')},\n",
       " 'Ridge': {'MAE': 18.855381894067843,\n",
       "  'MSE': 831.7957770846882,\n",
       "  'selected_feature': Index(['link_length', 'link_freespeed', 'link_capacity', 'link_permlanes',\n",
       "         'start_node_x', 'start_node_y', 'end_node_y', 'start_count',\n",
       "         'end_count'],\n",
       "        dtype='object')},\n",
       " 'SVR': {'MAE': 21.113999718501606,\n",
       "  'MSE': 998.1921690104024,\n",
       "  'selected_feature': Index(['link_id', 'link_length', 'link_freespeed', 'link_capacity',\n",
       "         'link_permlanes', 'start_node_x', 'start_node_y', 'end_node_x',\n",
       "         'end_node_y'],\n",
       "        dtype='object')},\n",
       " 'Random Forest': {'MAE': 16.64219759429435,\n",
       "  'MSE': 752.4379897068696,\n",
       "  'selected_feature': Index(['link_id', 'link_length', 'link_freespeed', 'link_capacity',\n",
       "         'link_permlanes', 'start_node_x', 'start_node_y', 'end_node_x',\n",
       "         'end_node_y'],\n",
       "        dtype='object')},\n",
       " 'Gradient Boosting': {'MAE': 17.670831896678532,\n",
       "  'MSE': 716.0204825248611,\n",
       "  'selected_feature': Index(['link_length', 'link_freespeed', 'link_capacity', 'link_permlanes',\n",
       "         'start_node_x', 'start_node_y', 'end_node_x', 'end_node_y',\n",
       "         'start_count'],\n",
       "        dtype='object')},\n",
       " 'Artificial Neural Network': {'MAE': 26.354717993482414,\n",
       "  'MSE': 1077.2371661655832,\n",
       "  'selected_feature': Index(['link_id', 'link_length', 'link_freespeed', 'link_capacity',\n",
       "         'link_permlanes', 'start_node_x', 'start_node_y', 'end_node_x',\n",
       "         'end_node_y'],\n",
       "        dtype='object')}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T15:58:29.829296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 70 candidates, totalling 210 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [-2.06753800e+21 -1.78347029e+03 -3.21274664e+21 -4.52036712e+58\n",
      " -2.11726699e+03 -1.78346984e+03 -1.78347029e+03 -1.78347029e+03\n",
      "             nan             nan -6.24658420e+19 -6.10801167e+12\n",
      " -1.96408398e+16 -1.78347029e+03             nan -1.78347029e+03\n",
      " -1.78347029e+03 -5.64219848e+20             nan -5.87544447e+21\n",
      "             nan -1.78347029e+03 -1.78347029e+03             nan\n",
      " -1.78347029e+03             nan -4.64578815e+15 -1.77140788e+03\n",
      "             nan -1.78347029e+03 -1.18411241e+16 -1.92220132e+03\n",
      " -1.78347029e+03 -1.78346995e+03             nan -2.11002684e+03\n",
      " -1.07400560e+17             nan             nan -1.78347029e+03\n",
      " -2.04274595e+03 -1.78347029e+03 -1.78347028e+03 -1.78347029e+03\n",
      " -1.78347029e+03 -1.78346908e+03 -1.77144765e+03 -1.78204420e+03\n",
      " -1.78347029e+03 -1.78347029e+03 -1.78347029e+03 -1.78070475e+03\n",
      " -2.11794703e+69 -1.78347029e+03 -1.78347029e+03 -2.47674823e+13\n",
      " -2.09596395e+22 -7.27577476e+20 -1.78347029e+03 -1.78347029e+03\n",
      " -5.24770013e+17 -2.66758547e+48             nan -8.75319761e+11\n",
      " -1.78347029e+03 -8.04400389e+14 -1.30787052e+23             nan\n",
      " -1.37463938e+10             nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 25.383309585489613, 'epsilon': 0.01, 'gamma': 2.1969677491639246, 'kernel': 'rbf'}\n",
      "SVR(C=25.383309585489613, epsilon=0.01, gamma=2.1969677491639246, max_iter=2000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=2000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "param_grid_svr = {\n",
    "    'C': reciprocal(1e-4, 1e3),  # Extended range for the regularization parameter\n",
    "    'gamma': reciprocal(1e-4, 1e2),  # Including specific gamma values\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],  # Focusing on RBF kernel\n",
    "    'epsilon': [0.01, 0.1, 0.2],  # Epsilon in the epsilon-SVR model\n",
    "}\n",
    "\n",
    "random_search_svr = RandomizedSearchCV(SVR(max_iter=2000), param_grid_svr, n_iter=70, cv=3, n_jobs=-1, verbose=10, scoring='neg_mean_squared_error')\n",
    "random_search_svr.fit(X_t, y_t)\n",
    "print(random_search_svr.best_params_)\n",
    "print(random_search_svr.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-31T01:20:56.486270700Z",
     "start_time": "2023-12-31T01:19:49.061059700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
      "{'criterion': 'friedman_mse', 'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 150}\n",
      "RandomForestRegressor(criterion='friedman_mse', max_depth=20,\n",
      "                      min_samples_leaf=2, n_estimators=150, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2,4],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'criterion':['friedman_mse']\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(RandomForestRegressor(random_state=42), param_grid_rf, cv=3, n_jobs=-1, verbose=10, scoring='neg_mean_squared_error')\n",
    "grid_search_rf.fit(X_t, y_t)\n",
    "print(grid_search_rf.best_params_)\n",
    "print(grid_search_rf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-31T14:55:59.094098400Z",
     "start_time": "2023-12-31T14:48:54.215646500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.8}\n",
      "GradientBoostingRegressor(max_depth=5, min_samples_split=5, random_state=42,\n",
      "                          subsample=0.8)\n"
     ]
    }
   ],
   "source": [
    "param_grid_gb = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],  # Varied learning rates for gradient boosting\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 4],\n",
    "    'subsample': [0.8, 1.0],  # Fraction of samples to be used for fitting individual base learners\n",
    "}\n",
    "\n",
    "grid_search_gb = GridSearchCV(GradientBoostingRegressor(random_state=42), param_grid_gb, cv=3, n_jobs=-1, verbose=5, scoring='neg_mean_squared_error')\n",
    "grid_search_gb.fit(X_t, y_t)\n",
    "print(grid_search_gb.best_params_)\n",
    "print(grid_search_gb.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T19:56:11.590867600Z",
     "start_time": "2024-01-02T19:50:33.301610200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 192 candidates, totalling 576 fits\n",
      "{'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'learning_rate_init': 0.01, 'solver': 'sgd'}\n",
      "MLPRegressor(activation='tanh', alpha=0.001, learning_rate_init=0.01,\n",
      "             max_iter=1000, random_state=42, solver='sgd')\n"
     ]
    }
   ],
   "source": [
    "param_grid_mlp = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.001, 0.01, 0.1],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "    'learning_rate_init': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "grid_search_mlp = GridSearchCV(MLPRegressor(max_iter=1000, random_state=42), param_grid_mlp, cv=3, n_jobs=-1, verbose=5, scoring='neg_mean_squared_error')\n",
    "grid_search_mlp.fit(X_t, y_t)\n",
    "print(grid_search_mlp.best_params_)\n",
    "print(grid_search_mlp.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-01T21:55:20.525741600Z",
     "start_time": "2024-01-01T20:54:36.357249300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "9 fits failed out of a total of 15.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 310, in fit\n",
      "    self.L_ = cholesky(K, lower=GPR_CHOLESKY_LOWER, check_finite=False)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: (\"The kernel, DotProduct(sigma_0=10), is not returning a positive definite matrix. Try gradually increasing the 'alpha' parameter of your GaussianProcessRegressor estimator.\", '8-th leading minor of the array is not positive definite')\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 310, in fit\n",
      "    self.L_ = cholesky(K, lower=GPR_CHOLESKY_LOWER, check_finite=False)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: (\"The kernel, DotProduct(sigma_0=10), is not returning a positive definite matrix. Try gradually increasing the 'alpha' parameter of your GaussianProcessRegressor estimator.\", '13-th leading minor of the array is not positive definite')\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 310, in fit\n",
      "    self.L_ = cholesky(K, lower=GPR_CHOLESKY_LOWER, check_finite=False)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: (\"The kernel, DotProduct(sigma_0=10), is not returning a positive definite matrix. Try gradually increasing the 'alpha' parameter of your GaussianProcessRegressor estimator.\", '17-th leading minor of the array is not positive definite')\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 310, in fit\n",
      "    self.L_ = cholesky(K, lower=GPR_CHOLESKY_LOWER, check_finite=False)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: (\"The kernel, DotProduct(sigma_0=0.1), is not returning a positive definite matrix. Try gradually increasing the 'alpha' parameter of your GaussianProcessRegressor estimator.\", '8-th leading minor of the array is not positive definite')\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 310, in fit\n",
      "    self.L_ = cholesky(K, lower=GPR_CHOLESKY_LOWER, check_finite=False)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: (\"The kernel, DotProduct(sigma_0=0.1), is not returning a positive definite matrix. Try gradually increasing the 'alpha' parameter of your GaussianProcessRegressor estimator.\", '13-th leading minor of the array is not positive definite')\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [           nan            nan -2092.47387429            nan\n",
      " -2083.41356604]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kernel': RBF(length_scale=10), 'alpha': 0.01}\n",
      "GaussianProcessRegressor(alpha=0.01, kernel=RBF(length_scale=10))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel, DotProduct\n",
    "import numpy as np\n",
    "param_grid = [{\n",
    "    \"alpha\":  [1e-2, 1e-3],\n",
    "    \"kernel\": [RBF(l) for l in np.logspace(-1, 1, 2)]\n",
    "}, {\n",
    "    \"alpha\":  [1e-2, 1e-3],\n",
    "    \"kernel\": [DotProduct(sigma_0) for sigma_0 in np.logspace(-1, 1, 2)]\n",
    "}]\n",
    "\n",
    "gpr = GaussianProcessRegressor()\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_gpr = RandomizedSearchCV(gpr, param_grid, n_iter=5, cv=3, scoring='neg_mean_squared_error', n_jobs=3, verbose=10)\n",
    "grid_search_gpr.fit(X_t, y_t)\n",
    "\n",
    "print(grid_search_gpr.best_params_)\n",
    "print(grid_search_gpr.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
