{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold, RandomizedSearchCV, StratifiedKFold\n",
    "from scipy.stats import expon, reciprocal, uniform\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, DotProduct, ExpSineSquared, RationalQuadratic\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFE, SelectFromModel, RFECV\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T19:49:55.184659400Z",
     "start_time": "2024-01-02T19:49:39.702704300Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(file_list, df_activities, df_links_network):\n",
    "    data_frames = []\n",
    "    for file in file_list:\n",
    "        with open(file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            df_links = pd.DataFrame({\n",
    "                'link_id': data['links_id'],\n",
    "                'link_from': data['link_from'],\n",
    "                'link_to': data['link_to'],\n",
    "                'link_length': data['link_length'],\n",
    "                'link_freespeed': data['link_freespeed'],\n",
    "                'link_capacity': data['link_capacity'],\n",
    "                'link_permlanes': data['link_permlanes'],\n",
    "                'link_counts': data['link_counts']\n",
    "            })\n",
    "            df_nodes = pd.DataFrame({\n",
    "                'node_id': data['nodes_id'],\n",
    "                'node_x': data['nodes_x'],\n",
    "                'node_y': data['nodes_y']\n",
    "            })\n",
    "            df_od_pairs = pd.DataFrame(data['o_d_pairs'], columns=['origin', 'destination'])\n",
    "            \n",
    "            df_work = pd.DataFrame({\n",
    "                        'work_x': data['work_x'],\n",
    "                        'work_y': data['work_y'],\n",
    "                        'go_to_work': data['go_to_work']\n",
    "            })\n",
    "            df_home = pd.DataFrame({\n",
    "                'home_x': data['home_x'],\n",
    "                'home_y': data['home_y'],\n",
    "                'go_to_home': data['go_to_home']\n",
    "            })\n",
    "            \n",
    "            df_links = df_links.merge(df_nodes, how='left', left_on='link_from', right_on='node_id')\n",
    "            df_links = df_links.rename(columns={'node_x': 'start_node_x', 'node_y': 'start_node_y'})\n",
    "            df_links.drop('node_id', axis=1, inplace=True)\n",
    "            df_links = df_links.merge(df_nodes, how='left', left_on='link_to', right_on='node_id')\n",
    "            df_links = df_links.rename(columns={'node_x': 'end_node_x', 'node_y': 'end_node_y'})\n",
    "            df_links.drop('node_id', axis=1, inplace=True) \n",
    "            \n",
    "            origin_counts = df_od_pairs['origin'].value_counts()\n",
    "            df_origin_counts = origin_counts.reset_index()\n",
    "            df_origin_counts.columns = ['origin', 'start_count']\n",
    "            destination_counts = df_od_pairs['destination'].value_counts()\n",
    "            df_destination_counts = destination_counts.reset_index()\n",
    "            df_destination_counts.columns = ['destination', 'end_count']\n",
    "            df_links = df_links.merge(df_origin_counts, how='left', left_on='link_from', right_on='origin')\n",
    "            df_links.drop('origin', axis=1, inplace=True)\n",
    "            df_links = df_links.merge(df_destination_counts, how='left', left_on='link_to', right_on='destination')\n",
    "            df_links.drop('destination', axis=1, inplace=True)\n",
    "            df_links[['start_count','end_count']] = df_links[['start_count','end_count']].fillna(0)\n",
    "            \n",
    "            df_act_work = df_activities[df_activities['activity_type_main']=='work']\n",
    "            df_act_work = df_act_work.merge(df_work, how='left', left_on=['x','y'], right_on=['work_x','work_y'])\n",
    "            df_act_work.drop(['x','y'], axis=1, inplace=True)\n",
    "            df_act_work_agg = df_act_work.groupby(by=\"link\").sum()['go_to_work'].reset_index(drop=False)\n",
    "            df_act_home = df_activities[df_activities['activity_type_main']=='home']\n",
    "            df_act_home = df_act_home.merge(df_home, how='left', left_on=['x','y'], right_on=['home_x','home_y'])\n",
    "            df_act_home.drop(['x','y'], axis=1, inplace=True)\n",
    "            df_act_home_agg = df_act_home.groupby(by=\"link\").sum()['go_to_home'].reset_index(drop=False)\n",
    "            df_act_agg = df_act_home_agg.merge(df_act_work_agg, how='outer', on='link')\n",
    "            df_act_agg.fillna(0, inplace=True)\n",
    "            df_act_agg['go_to_sum'] = df_act_agg['go_to_home'] + df_act_agg['go_to_work']\n",
    "            \n",
    "            mg = df_links.merge(df_links_network, how='left', on=['start_node_x','start_node_y','end_node_x','end_node_y'])\n",
    "            mg = mg[['link_id_x','link_from','link_to','link_id_y','from', 'to']]\n",
    "            link_home_work = mg.merge(df_act_agg, how='left', left_on='link_id_y', right_on='link')\n",
    "            link_home_work['go_to_sum'].fillna(0, inplace=True)\n",
    "            link_go_to = link_home_work[['link_id_x', 'go_to_sum']]\n",
    "            df_links = df_links.merge(link_go_to, how='left', left_on='link_id', right_on='link_id_x')\n",
    "            df_links.drop('link_id_x', axis=1, inplace=True)\n",
    "        data_frames.append(df_links)\n",
    "    return pd.concat(data_frames, ignore_index=True)\n",
    "# \n",
    "# \n",
    "train_files = ['s-0.json', 's-1.json', 's-2.json', 's-3.json', 's-4.json','s-5.json', 's-6.json', 's-7.json', 's-8.json', 's-9.json'] \n",
    "test_files = ['s-15.json', 's-16.json', 's-17.json', 's-18.json','s-19.json']\n",
    "validate_files = ['s-10.json', 's-11.json', 's-12.json', 's-13.json','s-14.json']\n",
    "train_files = ['Data/cutoutWorlds/Train/po-1_pn-1.0_sn-1/' + i for i in train_files]\n",
    "test_files = ['Data/cutoutWorlds/Test/po-1_pn-1.0_sn-1/' + j for j in test_files]\n",
    "validate_files = ['Data/cutoutWorlds/Validate/po-1_pn-1.0_sn-1/' + k for k in validate_files]\n",
    "df_activities = pd.read_pickle(\"Data/cutoutWorlds/Train/po-1_pn-1.0_sn-1/df_activities.pkl\")\n",
    "df_links_network = pd.read_pickle(\"Data/cutoutWorlds/Train/po-1_pn-1.0_sn-1/df_links_network.pkl\")\n",
    "train_data = load_data(train_files, df_activities, df_links_network)\n",
    "test_data = load_data(test_files, df_activities, df_links_network)\n",
    "validate_data = load_data(validate_files, df_activities, df_links_network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T20:01:05.026429300Z",
     "start_time": "2023-12-29T20:01:04.818365100Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Initialize a list to hold trips\n",
    "# trips = []\n",
    "# current_trip = [df_od_pairs.iloc[0]['origin']]  # Start with the first origin\n",
    "# \n",
    "# # Iterate over the DataFrame rows\n",
    "# for i, row in df_od_pairs.iterrows():\n",
    "#     current_trip.append(row['destination'])  # Always add the destination\n",
    "#     # Check if the next origin matches the current destination\n",
    "#     if i + 1 < len(df_od_pairs) and row['destination'] != df_od_pairs.iloc[i + 1]['origin']:\n",
    "#         # If it doesn't, the current trip has ended\n",
    "#         trips.append(current_trip)\n",
    "#         current_trip = [df_od_pairs.iloc[i + 1]['origin']]  # Start a new trip\n",
    "# \n",
    "# # Add the last trip if it wasn't already added\n",
    "# if current_trip not in trips:\n",
    "#     trips.append(current_trip)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-29T20:01:05.028445Z",
     "start_time": "2023-12-29T20:01:04.975096700Z"
    }
   },
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# # Flatten the list of trips into a single list of nodes including origins and destinations\n",
    "# all_nodes = [node for trip in trips for node in trip]\n",
    "# \n",
    "# # Use Counter to count the occurrences of each node\n",
    "# node_trip_counts = Counter(all_nodes)\n",
    "# \n",
    "# df_node_trip_counts = pd.DataFrame.from_dict(node_trip_counts, orient='index').reset_index()\n",
    "# df_node_trip_counts.columns = ['node_id', 'trip_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T15:44:41.533491700Z",
     "start_time": "2024-01-02T15:44:41.482248800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16985"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T19:49:55.235792300Z",
     "start_time": "2024-01-02T19:49:55.191146500Z"
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "numerical_features = ['link_length', 'link_freespeed', 'link_capacity', 'link_permlanes', 'start_node_x', 'start_node_y', 'end_node_x', 'end_node_y', 'start_count', 'end_count', 'go_to_sum']\n",
    "X_t = train_data.drop(columns=['link_counts'])\n",
    "y_t = train_data['link_counts']\n",
    "X_v = validate_data.drop(columns=['link_counts'])\n",
    "y_v = validate_data['link_counts']\n",
    "X_te = test_data.drop(columns=['link_counts'])\n",
    "y_te = test_data['link_counts']\n",
    "scaler = StandardScaler()\n",
    "X_t[numerical_features] = scaler.fit_transform(X_t[numerical_features])\n",
    "X_v[numerical_features] = scaler.fit_transform(X_v[numerical_features])\n",
    "X_te[numerical_features] = scaler.fit_transform(X_te[numerical_features])\n",
    "# X_t = scaler.fit_transform(X_t)\n",
    "# X_v = scaler.fit_transform(X_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T15:58:27.133238400Z",
     "start_time": "2024-01-02T15:58:16.618137100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LinBoH\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LinBoH\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LinBoH\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LinBoH\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "C:\\Users\\LinBoH\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:299: ConvergenceWarning: Solver terminated early (max_iter=3000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LinBoH\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LinBoH\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LinBoH\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial Neural Network done\n",
      "\n",
      "\n",
      "Model: Linear Regression\n",
      "MAE: 19.145577239419847\n",
      "MSE: 846.8004311931436\n",
      "selected_feature: Index(['link_length', 'link_freespeed', 'link_capacity', 'link_permlanes',\n",
      "       'start_node_x', 'start_node_y', 'end_node_x', 'end_node_y',\n",
      "       'start_count', 'end_count', 'go_to_sum'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: Lasso\n",
      "MAE: 19.556427358308127\n",
      "MSE: 867.8803264170302\n",
      "selected_feature: Index(['link_freespeed', 'link_capacity', 'link_permlanes', 'start_node_y',\n",
      "       'start_count', 'end_count'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: Ridge\n",
      "MAE: 19.61008037407602\n",
      "MSE: 870.7221155998428\n",
      "selected_feature: Index(['link_freespeed', 'link_capacity', 'link_permlanes', 'start_node_y',\n",
      "       'end_node_y', 'start_count', 'end_count'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: SVR\n",
      "MAE: 22.026074260759366\n",
      "MSE: 1017.7208956151288\n",
      "selected_feature: Index(['link_id', 'link_to', 'link_length', 'link_freespeed', 'link_capacity',\n",
      "       'link_permlanes', 'start_node_x', 'start_node_y', 'end_node_x',\n",
      "       'end_node_y', 'start_count'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: Random Forest\n",
      "MAE: 18.425905436509957\n",
      "MSE: 881.8537212609749\n",
      "selected_feature: Index(['link_freespeed', 'link_capacity', 'link_permlanes', 'start_node_y',\n",
      "       'end_node_x', 'end_node_y'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: Gradient Boosting\n",
      "MAE: 15.226484039945586\n",
      "MSE: 613.1150491644793\n",
      "selected_feature: Index(['link_id', 'link_to', 'link_length', 'link_freespeed', 'link_capacity',\n",
      "       'link_permlanes', 'start_node_x', 'start_node_y', 'end_node_x',\n",
      "       'end_node_y', 'start_count', 'end_count'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Model: Artificial Neural Network\n",
      "MAE: 26.354717993482414\n",
      "MSE: 1077.2371661655832\n",
      "selected_feature: Index(['link_id', 'link_length', 'link_freespeed', 'link_capacity',\n",
      "       'link_permlanes', 'start_node_x', 'start_node_y', 'end_node_x',\n",
      "       'end_node_y'],\n",
      "      dtype='object')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Lasso': LassoCV(cv=3, random_state=42, max_iter=100000),\n",
    "    'Ridge': RidgeCV(cv=3),\n",
    "    'SVR': SVR(C=25.383309585489613, epsilon=0.01, gamma=2.1969677491639246, max_iter=3000),\n",
    "    'Random Forest': RandomForestRegressor(criterion='friedman_mse', max_depth=20, min_samples_leaf=2, n_estimators=150, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(max_depth=5, min_samples_split=5, random_state=42, subsample=0.8),\n",
    "    'Artificial Neural Network': MLPRegressor(activation='tanh', alpha=0.001, learning_rate_init=0.01, max_iter=1000, random_state=42, solver='sgd'),\n",
    "#     'Gaussian Process Regression': GaussianProcessRegressor(kernel=RBF(length_scale=1.0) + WhiteKernel(noise_level=1.0), alpha=0.1, n_restarts_optimizer=3)\n",
    "}\n",
    "\n",
    "# Function to train and evaluate models\n",
    "def evaluate_models(models, X_train, y_train, X_test, y_test):\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "#         kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "#         cv_scores = cross_val_score(model, X_train, y_train, cv=kf, scoring='neg_mean_squared_error')\n",
    "#         mse_scores = -cv_scores \n",
    "#         mean_mse = mse_scores.mean()\n",
    "#         std_mse = mse_scores.std()\n",
    "        # mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "        # r2 = r2_score(y_test, y_pred)\n",
    "        print(name + \" done\")\n",
    "        \n",
    "        results[name] = {'MAE': mae, 'MSE': mse}\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Function to train and evaluate models\n",
    "def feature_select_models(models, X_train, y_train, X_test, y_test):\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        if name not in ['SVR', 'Artificial Neural Network', 'Gaussian Process Regression']:\n",
    "            selector = RFECV(model, step=1, cv=StratifiedKFold(5), scoring='neg_mean_squared_error').fit(X_train, y_train)\n",
    "            # To see which features are selected\n",
    "            selected_features = X_train.columns[selector.support_]\n",
    "            y_pred = selector.predict(X_test)\n",
    "        else:\n",
    "            # Fit Random Forest to get feature importances\n",
    "            rf = RandomForestRegressor()\n",
    "            rf.fit(X_train, y_train)\n",
    "\n",
    "            # Select features based on importances\n",
    "            rfe = RFECV(estimator=rf, step=1, cv=StratifiedKFold(5), scoring='neg_mean_squared_error').fit(X_train, y_train)\n",
    "            selected_features = X_train.columns[rfe.support_]\n",
    "            X_train_reduced = X_train[selected_features]\n",
    "            X_test_reduced = X_test[selected_features]\n",
    "            model.fit(X_train_reduced, y_train)\n",
    "            y_pred = model.predict(X_test_reduced)\n",
    "       \n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        print(name + \" done\")\n",
    "\n",
    "        results[name] = {'MAE': mae, 'MSE': mse, 'selected_feature': selected_features}\n",
    "\n",
    "            \n",
    "    return results\n",
    "\n",
    "# Train and evaluate\n",
    "# results = evaluate_models(models, X_t, y_t, X_v, y_v)\n",
    "\n",
    "# # Display Results\n",
    "# for model_name, metrics in results.items():\n",
    "#     print(\"\\n\")\n",
    "#     print(f\"Model: {model_name}\")\n",
    "#     for metric_name, value in metrics.items():\n",
    "#         print(f\"{metric_name}: {value}\")\n",
    "#     print(\"\\n\")\n",
    "\n",
    "results_feature = feature_select_models(models, X_t, y_t, X_te, y_te)\n",
    "for model_name, metrics in results_feature.items():\n",
    "    print(\"\\n\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"{metric_name}: {value}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('result_after_featureselection.pkl', 'wb') as file:\n",
    "    pickle.dump(results_feature, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Linear Regression': {'MAE': 19.000541966353587, 'MSE': 1350.56069460534},\n",
       " 'Lasso': {'MAE': 22.610703312854127, 'MSE': 1679.4760262080995},\n",
       " 'Ridge': {'MAE': 18.989592790164192, 'MSE': 1349.7480663335148},\n",
       " 'SVR': {'MAE': 23.793351948629688, 'MSE': 1756.1078902638878},\n",
       " 'Random Forest': {'MAE': 17.635830260141653, 'MSE': 2015.2344103327919},\n",
       " 'Gradient Boosting': {'MAE': 15.969274342418169, 'MSE': 1596.9989870917648},\n",
       " 'Artificial Neural Network': {'MAE': 21.345942437519778,\n",
       "  'MSE': 1888.5103715269568}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Linear Regression': {'MAE': 19.47148762371669,\n",
       "  'MSE': 874.7590684153923,\n",
       "  'selected_feature': Index(['link_length', 'link_freespeed', 'link_capacity', 'link_permlanes',\n",
       "         'start_node_x', 'start_node_y', 'end_node_x', 'end_node_y',\n",
       "         'start_count'],\n",
       "        dtype='object')},\n",
       " 'Lasso': {'MAE': 21.29080224423866,\n",
       "  'MSE': 995.8290100956449,\n",
       "  'selected_feature': Index(['link_id', 'link_permlanes', 'start_node_x', 'start_node_y',\n",
       "         'end_node_x', 'end_node_y', 'start_count', 'end_count', 'go_to_sum'],\n",
       "        dtype='object')},\n",
       " 'Ridge': {'MAE': 18.855381894067843,\n",
       "  'MSE': 831.7957770846882,\n",
       "  'selected_feature': Index(['link_length', 'link_freespeed', 'link_capacity', 'link_permlanes',\n",
       "         'start_node_x', 'start_node_y', 'end_node_y', 'start_count',\n",
       "         'end_count'],\n",
       "        dtype='object')},\n",
       " 'SVR': {'MAE': 21.113999718501606,\n",
       "  'MSE': 998.1921690104024,\n",
       "  'selected_feature': Index(['link_id', 'link_length', 'link_freespeed', 'link_capacity',\n",
       "         'link_permlanes', 'start_node_x', 'start_node_y', 'end_node_x',\n",
       "         'end_node_y'],\n",
       "        dtype='object')},\n",
       " 'Random Forest': {'MAE': 16.64219759429435,\n",
       "  'MSE': 752.4379897068696,\n",
       "  'selected_feature': Index(['link_id', 'link_length', 'link_freespeed', 'link_capacity',\n",
       "         'link_permlanes', 'start_node_x', 'start_node_y', 'end_node_x',\n",
       "         'end_node_y'],\n",
       "        dtype='object')},\n",
       " 'Gradient Boosting': {'MAE': 17.670831896678532,\n",
       "  'MSE': 716.0204825248611,\n",
       "  'selected_feature': Index(['link_length', 'link_freespeed', 'link_capacity', 'link_permlanes',\n",
       "         'start_node_x', 'start_node_y', 'end_node_x', 'end_node_y',\n",
       "         'start_count'],\n",
       "        dtype='object')},\n",
       " 'Artificial Neural Network': {'MAE': 26.354717993482414,\n",
       "  'MSE': 1077.2371661655832,\n",
       "  'selected_feature': Index(['link_id', 'link_length', 'link_freespeed', 'link_capacity',\n",
       "         'link_permlanes', 'start_node_x', 'start_node_y', 'end_node_x',\n",
       "         'end_node_y'],\n",
       "        dtype='object')}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-02T15:58:29.829296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 70 candidates, totalling 210 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [-2.06753800e+21 -1.78347029e+03 -3.21274664e+21 -4.52036712e+58\n",
      " -2.11726699e+03 -1.78346984e+03 -1.78347029e+03 -1.78347029e+03\n",
      "             nan             nan -6.24658420e+19 -6.10801167e+12\n",
      " -1.96408398e+16 -1.78347029e+03             nan -1.78347029e+03\n",
      " -1.78347029e+03 -5.64219848e+20             nan -5.87544447e+21\n",
      "             nan -1.78347029e+03 -1.78347029e+03             nan\n",
      " -1.78347029e+03             nan -4.64578815e+15 -1.77140788e+03\n",
      "             nan -1.78347029e+03 -1.18411241e+16 -1.92220132e+03\n",
      " -1.78347029e+03 -1.78346995e+03             nan -2.11002684e+03\n",
      " -1.07400560e+17             nan             nan -1.78347029e+03\n",
      " -2.04274595e+03 -1.78347029e+03 -1.78347028e+03 -1.78347029e+03\n",
      " -1.78347029e+03 -1.78346908e+03 -1.77144765e+03 -1.78204420e+03\n",
      " -1.78347029e+03 -1.78347029e+03 -1.78347029e+03 -1.78070475e+03\n",
      " -2.11794703e+69 -1.78347029e+03 -1.78347029e+03 -2.47674823e+13\n",
      " -2.09596395e+22 -7.27577476e+20 -1.78347029e+03 -1.78347029e+03\n",
      " -5.24770013e+17 -2.66758547e+48             nan -8.75319761e+11\n",
      " -1.78347029e+03 -8.04400389e+14 -1.30787052e+23             nan\n",
      " -1.37463938e+10             nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 25.383309585489613, 'epsilon': 0.01, 'gamma': 2.1969677491639246, 'kernel': 'rbf'}\n",
      "SVR(C=25.383309585489613, epsilon=0.01, gamma=2.1969677491639246, max_iter=2000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=2000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "param_grid_svr = {\n",
    "    'C': reciprocal(1e-4, 1e3),  # Extended range for the regularization parameter\n",
    "    'gamma': reciprocal(1e-4, 1e2),  # Including specific gamma values\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],  # Focusing on RBF kernel\n",
    "    'epsilon': [0.01, 0.1, 0.2],  # Epsilon in the epsilon-SVR model\n",
    "}\n",
    "\n",
    "random_search_svr = RandomizedSearchCV(SVR(max_iter=2000), param_grid_svr, n_iter=70, cv=3, n_jobs=-1, verbose=10, scoring='neg_mean_squared_error')\n",
    "random_search_svr.fit(X_t, y_t)\n",
    "print(random_search_svr.best_params_)\n",
    "print(random_search_svr.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-31T01:20:56.486270700Z",
     "start_time": "2023-12-31T01:19:49.061059700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n",
      "{'criterion': 'friedman_mse', 'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 150}\n",
      "RandomForestRegressor(criterion='friedman_mse', max_depth=20,\n",
      "                      min_samples_leaf=2, n_estimators=150, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 150, 200],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2,4],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'criterion':['friedman_mse']\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(RandomForestRegressor(random_state=42), param_grid_rf, cv=3, n_jobs=-1, verbose=10, scoring='neg_mean_squared_error')\n",
    "grid_search_rf.fit(X_t, y_t)\n",
    "print(grid_search_rf.best_params_)\n",
    "print(grid_search_rf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-31T14:55:59.094098400Z",
     "start_time": "2023-12-31T14:48:54.215646500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 144 candidates, totalling 432 fits\n",
      "{'learning_rate': 0.1, 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100, 'subsample': 0.8}\n",
      "GradientBoostingRegressor(max_depth=5, min_samples_split=5, random_state=42,\n",
      "                          subsample=0.8)\n"
     ]
    }
   ],
   "source": [
    "param_grid_gb = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],  # Varied learning rates for gradient boosting\n",
    "    'max_depth': [3, 5, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 4],\n",
    "    'subsample': [0.8, 1.0],  # Fraction of samples to be used for fitting individual base learners\n",
    "}\n",
    "\n",
    "grid_search_gb = GridSearchCV(GradientBoostingRegressor(random_state=42), param_grid_gb, cv=3, n_jobs=-1, verbose=5, scoring='neg_mean_squared_error')\n",
    "grid_search_gb.fit(X_t, y_t)\n",
    "print(grid_search_gb.best_params_)\n",
    "print(grid_search_gb.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-02T19:56:11.590867600Z",
     "start_time": "2024-01-02T19:50:33.301610200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 192 candidates, totalling 576 fits\n",
      "{'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'learning_rate_init': 0.01, 'solver': 'sgd'}\n",
      "MLPRegressor(activation='tanh', alpha=0.001, learning_rate_init=0.01,\n",
      "             max_iter=1000, random_state=42, solver='sgd')\n"
     ]
    }
   ],
   "source": [
    "param_grid_mlp = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.001, 0.01, 0.1],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "    'learning_rate_init': [0.001, 0.01]\n",
    "}\n",
    "\n",
    "grid_search_mlp = GridSearchCV(MLPRegressor(max_iter=1000, random_state=42), param_grid_mlp, cv=3, n_jobs=-1, verbose=5, scoring='neg_mean_squared_error')\n",
    "grid_search_mlp.fit(X_t, y_t)\n",
    "print(grid_search_mlp.best_params_)\n",
    "print(grid_search_mlp.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-01T21:55:20.525741600Z",
     "start_time": "2024-01-01T20:54:36.357249300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "9 fits failed out of a total of 15.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 310, in fit\n",
      "    self.L_ = cholesky(K, lower=GPR_CHOLESKY_LOWER, check_finite=False)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: (\"The kernel, DotProduct(sigma_0=10), is not returning a positive definite matrix. Try gradually increasing the 'alpha' parameter of your GaussianProcessRegressor estimator.\", '8-th leading minor of the array is not positive definite')\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 310, in fit\n",
      "    self.L_ = cholesky(K, lower=GPR_CHOLESKY_LOWER, check_finite=False)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: (\"The kernel, DotProduct(sigma_0=10), is not returning a positive definite matrix. Try gradually increasing the 'alpha' parameter of your GaussianProcessRegressor estimator.\", '13-th leading minor of the array is not positive definite')\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 310, in fit\n",
      "    self.L_ = cholesky(K, lower=GPR_CHOLESKY_LOWER, check_finite=False)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: (\"The kernel, DotProduct(sigma_0=10), is not returning a positive definite matrix. Try gradually increasing the 'alpha' parameter of your GaussianProcessRegressor estimator.\", '17-th leading minor of the array is not positive definite')\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 310, in fit\n",
      "    self.L_ = cholesky(K, lower=GPR_CHOLESKY_LOWER, check_finite=False)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: (\"The kernel, DotProduct(sigma_0=0.1), is not returning a positive definite matrix. Try gradually increasing the 'alpha' parameter of your GaussianProcessRegressor estimator.\", '8-th leading minor of the array is not positive definite')\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\gaussian_process\\_gpr.py\", line 310, in fit\n",
      "    self.L_ = cholesky(K, lower=GPR_CHOLESKY_LOWER, check_finite=False)\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py\", line 88, in cholesky\n",
      "    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n",
      "  File \"C:\\Users\\user\\anaconda3\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py\", line 37, in _cholesky\n",
      "    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\n",
      "numpy.linalg.LinAlgError: (\"The kernel, DotProduct(sigma_0=0.1), is not returning a positive definite matrix. Try gradually increasing the 'alpha' parameter of your GaussianProcessRegressor estimator.\", '13-th leading minor of the array is not positive definite')\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [           nan            nan -2092.47387429            nan\n",
      " -2083.41356604]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kernel': RBF(length_scale=10), 'alpha': 0.01}\n",
      "GaussianProcessRegressor(alpha=0.01, kernel=RBF(length_scale=10))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel, DotProduct\n",
    "import numpy as np\n",
    "param_grid = [{\n",
    "    \"alpha\":  [1e-2, 1e-3],\n",
    "    \"kernel\": [RBF(l) for l in np.logspace(-1, 1, 2)]\n",
    "}, {\n",
    "    \"alpha\":  [1e-2, 1e-3],\n",
    "    \"kernel\": [DotProduct(sigma_0) for sigma_0 in np.logspace(-1, 1, 2)]\n",
    "}]\n",
    "\n",
    "gpr = GaussianProcessRegressor()\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_gpr = RandomizedSearchCV(gpr, param_grid, n_iter=5, cv=3, scoring='neg_mean_squared_error', n_jobs=3, verbose=10)\n",
    "grid_search_gpr.fit(X_t, y_t)\n",
    "\n",
    "print(grid_search_gpr.best_params_)\n",
    "print(grid_search_gpr.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
