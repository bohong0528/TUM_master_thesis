{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-05T00:19:43.963037300Z",
     "start_time": "2024-01-05T00:19:43.928475600Z"
    },
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LinBoH\\AppData\\Local\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, MinMaxScaler, RobustScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV, LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error, make_scorer, max_error, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, KFold, RandomizedSearchCV, ShuffleSplit, cross_validate, train_test_split\n",
    "from scipy.stats import expon, reciprocal, uniform\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, DotProduct, ExpSineSquared, RationalQuadratic, ConstantKernel, Matern\n",
    "from sklearn.feature_selection import RFE, SelectFromModel, RFECV, SelectKBest, chi2, f_regression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from mango import Tuner, scheduler\n",
    "import xgboost as xgb\n",
    "from skopt  import BayesSearchCV \n",
    "import lightgbm as lgb\n",
    "from sklearn.cluster import OPTICS, MiniBatchKMeans\n",
    "from pyGRNN import GRNN\n",
    "from skopt.space import Categorical, Space, Dimension, Integer\n",
    "from sklearn.inspection import permutation_importance\n",
    "from optuna.integration import OptunaSearchCV\n",
    "import optuna\n",
    "import matplotlib.pyplot as plt\n",
    "from loading import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['start_node_x', 'start_node_y', 'end_node_x', 'end_node_y', 'link_length', 'link_freespeed', \n",
    "                      'link_capacity', 'link_permlanes', 'start_count', 'end_count', 'go_to_sum', 'rush_hour', \n",
    "                      'max_dur', 'cemdapStopDuration_s', 'length_per_capacity_ratio', 'speed_capacity_ratio',\n",
    "                      'length_times_lanes', 'speed_times_capacity', 'length_times', 'capacity_divided_by_lanes',\n",
    "                      'income', 'score', 'income_avg', 'score_avg'\n",
    "                     ]\n",
    "category_feature = ['type', 'home-activity-zone']\n",
    "scaler = StandardScaler()\n",
    "le = LabelEncoder()\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "ct = ColumnTransformer(\n",
    "     [(\"num_preprocess\", scaler, numerical_features),\n",
    "      (\"text_preprocess\", ohe, category_feature)], remainder='passthrough').set_output(transform=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_files = ['s-0.json', 's-1.json', 's-2.json', 's-3.json', 's-4.json','s-5.json', 's-6.json', 's-7.json', 's-8.json', 's-9.json'] \n",
    "test_files = ['s-15.json', 's-16.json', 's-17.json', 's-18.json','s-19.json']\n",
    "validate_files = ['s-10.json', 's-11.json', 's-12.json', 's-13.json','s-14.json']\n",
    "train_files = ['Data/cutoutWorlds/Train/po-1_pn-1.0_sn-1/' + i for i in train_files]\n",
    "test_files = ['Data/cutoutWorlds/Test/po-1_pn-1.0_sn-1/' + j for j in test_files]\n",
    "validate_files = ['Data/cutoutWorlds/Validate/po-1_pn-1.0_sn-1/' + k for k in validate_files]\n",
    "df_activities = pd.read_pickle(\"Data/cutoutWorlds/Train/po-1_pn-1.0_sn-1/df_activities.pkl\")\n",
    "df_links_network = pd.read_pickle(\"Data/cutoutWorlds/Train/po-1_pn-1.0_sn-1/df_links_network.pkl\")\n",
    "train_data = load_data(train_files, df_activities, df_links_network)\n",
    "validate_data = load_data(validate_files, df_activities, df_links_network)\n",
    "test_data = load_data(test_files, df_activities, df_links_network)\n",
    "train_data['dataset'] = 'train'\n",
    "validate_data['dataset'] = 'validate'\n",
    "test_data['dataset'] = 'test'\n",
    "Big_data = pd.concat([train_data, validate_data, test_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the indices where 'link_id' is 0\n",
    "indices = Big_data.index[Big_data['link_id'] == 0].tolist()\n",
    "\n",
    "# Add the end of the DataFrame to the indices list\n",
    "indices.append(len(Big_data))\n",
    "\n",
    "# Split the DataFrame using the indices\n",
    "dfs = [Big_data.iloc[indices[n]:indices[n+1]] for n in range(len(indices)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_od = []\n",
    "list_nodes = []\n",
    "all_files = train_files + validate_files + test_files\n",
    "for i in all_files:\n",
    "    with open(i) as f:\n",
    "        d = json.load(f)\n",
    "        list_od.append(d['o_d_pairs'])\n",
    "        list_nodes.append(d['nodes_id'])\n",
    "tuples_links = [ list(zip(dfs[i]['link_from'], dfs[i]['link_to'], dfs[i]['link_length'])) for i in range(20)]\n",
    "list_od_tuples = [[(origin, destination) for origin, destination in list_od[i]]for i in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Assume `nodes` is a list of all node IDs and `edges` is a list of tuples (start_node, end_node, weight)\n",
    "# For example:\n",
    "# nodes = [1, 2, 3, 4, ...]\n",
    "# edges = [(1, 2, 1.0), (2, 3, 2.5), (1, 3, 1.5), ...]\n",
    "# And `o_d_pairs` is a list of tuples representing the O-D pairs:\n",
    "# o_d_pairs = [(origin_1, destination_1), (origin_2, destination_2), ...]\n",
    "\n",
    "shortest_paths_list = []\n",
    "for i in range(20):\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(list_nodes[i])\n",
    "    G.add_weighted_edges_from(tuples_links[i])\n",
    "    shortest_paths = {}\n",
    "    for origin, destination in list_od_tuples[i]:\n",
    "        # This will find the shortest path by weight\n",
    "        try:\n",
    "            shortest_path = nx.shortest_path(G, source=origin, target=destination, weight='weight')\n",
    "        except:\n",
    "            shortest_path = []\n",
    "        shortest_paths[(origin, destination)] = shortest_path\n",
    "    shortest_paths_list.append(shortest_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "for i in range(20):\n",
    "    link_usage_counts = defaultdict(int)\n",
    "\n",
    "    # Iterate over each path and each link in the path\n",
    "    for path in shortest_paths_list[i].values():\n",
    "        for start_node, end_node in zip(path, path[1:]):\n",
    "            # Order the nodes to avoid counting (node1, node2) and (node2, node1) separately\n",
    "            ordered_link = tuple(sorted((start_node, end_node)))\n",
    "            link_usage_counts[ordered_link] += 1\n",
    "\n",
    "    # Now you have a dictionary with the count of usage for each link\n",
    "\n",
    "    # Assume you have a DataFrame 'links_df' with columns ['node_start', 'node_end']\n",
    "    # links_df = ...\n",
    "\n",
    "    # Add a 'used_count' column to your links data\n",
    "    dfs[i]['used_count'] = dfs[i].apply(\n",
    "        lambda row: link_usage_counts[tuple(sorted((row['link_from'], row['link_to'])))],\n",
    "        axis=1\n",
    "    )\n",
    "Big_data = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_data = Big_data[['link_id', 'start_node_x', 'start_node_y', 'end_node_x', 'end_node_y']]\n",
    "grouped = nodes_data.groupby(['start_node_x', 'start_node_y'])\n",
    "filtered_df = grouped.filter(lambda x: len(x) == 1)\n",
    "filtered_df = filtered_df.drop_duplicates()\n",
    "node_mapping = filtered_df.set_index(['start_node_x', 'start_node_y']).apply(\n",
    "    lambda row: (row['end_node_x'], row['end_node_y']), axis=1).to_dict()\n",
    "\n",
    "all_nodes = set(node_mapping.keys()) | set(node_mapping.values())\n",
    "end_nodes = set(node_mapping.values())\n",
    "\n",
    "start_nodes = list(all_nodes - end_nodes)\n",
    "\n",
    "paths = []\n",
    "for start_node in start_nodes:\n",
    "    path = [start_node]\n",
    "    while path[-1] in node_mapping:\n",
    "        next_node = node_mapping[path[-1]]\n",
    "        path.append(next_node)\n",
    "    paths.append(path)\n",
    "    \n",
    "new_paths = [x for x in paths if len(x) >2]\n",
    "def map_path_to_links(df, path):\n",
    "    path_links = pd.DataFrame()\n",
    "    for i in range(len(path) - 1):\n",
    "        start_node = path[i]\n",
    "        end_node = path[i+1]\n",
    "        link_row = df[(df['start_node_x'] == start_node[0]) & \n",
    "                      (df['start_node_y'] == start_node[1]) & \n",
    "                      (df['end_node_x'] == end_node[0]) & \n",
    "                      (df['end_node_y'] == end_node[1])]\n",
    "        if not link_row.empty:\n",
    "            path_links = pd.concat([path_links, link_row])\n",
    "    return path_links\n",
    "\n",
    "# Step 3: Create separate DataFrames for each path\n",
    "path_dfs = []\n",
    "for path in new_paths:\n",
    "    link_df = map_path_to_links(Big_data, path)\n",
    "    path_dfs.append(link_df)\n",
    "\n",
    "Big_data_drop = Big_data.copy(deep=True)\n",
    "for path_df in path_dfs:\n",
    "    numeric_df = path_df.select_dtypes(include=[ 'float64', 'int64'])\n",
    "    column_means = numeric_df.mean()\n",
    "    mean_df = pd.DataFrame([column_means])\n",
    "    zone = path_df['home-activity-zone'].mode()\n",
    "    type_value = path_df['type'].mode()\n",
    "    dataset = path_df['dataset'].mode()\n",
    "    mean_df['home-activity-zone'] = zone\n",
    "    mean_df['type'] = type_value\n",
    "    mean_df['dataset'] = dataset\n",
    "    Big_data_drop = pd.concat([Big_data_drop, mean_df])\n",
    "\n",
    "    try:\n",
    "        Big_data_drop.drop(path_df.index, inplace=True)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = MiniBatchKMeans(n_clusters=500, random_state=101)\n",
    "Big_data_drop['x_y_coor'] = cluster.fit_predict(Big_data_drop[['start_node_x', 'start_node_y',\n",
    "                                                           'end_node_x', 'end_node_y']])\n",
    "cluster1 = MiniBatchKMeans(n_clusters=500, random_state=101)\n",
    "Big_data_drop['similar_link'] = cluster1.fit_predict(Big_data_drop[['link_length', 'link_freespeed',\n",
    "                                                           'link_capacity', 'link_permlanes']])\n",
    "cluster2 = MiniBatchKMeans(n_clusters=500, random_state=101)\n",
    "Big_data_drop['planxml'] = cluster2.fit_predict(Big_data_drop[['income', 'score', 'rush_hour',\n",
    "                                                               'max_dur', 'cemdapStopDuration_s']])\n",
    "\n",
    "Big_data_drop = Big_data_drop.astype({'x_y_coor':'int64','similar_link':'int64', 'planxml':'int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Big_data_tr = ct.fit_transform(Big_data_drop)\n",
    "Big_data_tr['used_link'] = 1\n",
    "Big_data_tr['used_link'][Big_data_tr['remainder__link_counts']==0] = 0\n",
    "Big_data_tr = Big_data_tr.reset_index(drop=True)\n",
    "train_data_tr = Big_data_tr[Big_data_tr['remainder__dataset']=='train']\n",
    "validate_data_tr = Big_data_tr[Big_data_tr['remainder__dataset']=='validate']\n",
    "test_data_tr = Big_data_tr[Big_data_tr['remainder__dataset']=='test']\n",
    "\n",
    "train_index = list(train_data_tr.index)\n",
    "validate_index = list(validate_data_tr.index)\n",
    "\n",
    "temp = pd.concat([train_data_tr, validate_data_tr], ignore_index=True)\n",
    "temp = temp.astype({\"remainder__link_from\": int, \"remainder__link_to\": int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = list(temp.columns)\n",
    "nodes_features = ['remainder__link_from', 'remainder__link_to']\n",
    "drop_featrues = ['remainder__dataset', 'remainder__link_counts', 'used_link']\n",
    "temp_features = list(set(all_features) - set(nodes_features))\n",
    "other_features = list(set(temp_features) - set(drop_featrues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch.nn.functional as F\n",
    "class GATNet(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes,\n",
    "                hid, in_head, out_head, dor, extra_layer):\n",
    "        super(GATNet, self).__init__()\n",
    "        self.hid = hid\n",
    "        self.in_head = in_head\n",
    "        self.out_head = out_head\n",
    "        self.dor = dor\n",
    "        self.extra_layer = extra_layer\n",
    "        self.gat1 = GATConv(num_features, self.hid, heads=self.in_head, dropout=self.dor)\n",
    "        if self.extra_layer:\n",
    "            self.gat2 = GATConv(self.hid*self.in_head, self.hid, heads=self.in_head, dropout=self.dor)\n",
    "            self.gat3 = GATConv(self.hid*self.in_head, num_classes, concat=False, heads=self.out_head, dropout=self.dor)\n",
    "        else:\n",
    "            self.gat2 = GATConv(self.hid*self.in_head, num_classes, concat=False, heads=self.out_head, dropout=self.dor)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=self.dor, training=self.training)\n",
    "        x = F.elu(self.gat1(x, edge_index))\n",
    "        x = F.dropout(x, p=self.dor, training=self.training)\n",
    "        if self.extra_layer:\n",
    "            x = F.elu(self.gat2(x, edge_index))  # Add non-linearity after the second layer\n",
    "            x = F.dropout(x, p=self.dor, training=self.training)\n",
    "            x = self.gat3(x, edge_index) \n",
    "        else:\n",
    "            x = self.gat2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-13 23:09:44,471] A new study created in memory with name: no-name-7a495de2-8f58-456b-bad4-54a5885d76af\n",
      "[I 2024-03-13 23:10:07,880] Trial 0 finished with value: 24.233083724975586 and parameters: {'k': 39, 'hid': 64, 'in_head': 2, 'out_head': 2, 'dor': 0.1, 'extra_layer': True}. Best is trial 0 with value: 24.233083724975586.\n",
      "[I 2024-03-13 23:10:11,864] Trial 1 finished with value: 11.616863250732422 and parameters: {'k': 13, 'hid': 16, 'in_head': 2, 'out_head': 2, 'dor': 0.1, 'extra_layer': False}. Best is trial 1 with value: 11.616863250732422.\n",
      "[I 2024-03-13 23:19:47,091] Trial 2 finished with value: 16.875606536865234 and parameters: {'k': 29, 'hid': 256, 'in_head': 8, 'out_head': 2, 'dor': 0, 'extra_layer': True}. Best is trial 1 with value: 11.616863250732422.\n",
      "[I 2024-03-13 23:20:13,800] Trial 3 finished with value: 10.104442596435547 and parameters: {'k': 5, 'hid': 128, 'in_head': 4, 'out_head': 2, 'dor': 0.05, 'extra_layer': True}. Best is trial 3 with value: 10.104442596435547.\n",
      "[I 2024-03-13 23:20:36,481] Trial 4 finished with value: 10.938712120056152 and parameters: {'k': 21, 'hid': 64, 'in_head': 16, 'out_head': 1, 'dor': 0.1, 'extra_layer': False}. Best is trial 3 with value: 10.104442596435547.\n",
      "[I 2024-03-13 23:21:07,466] Trial 5 finished with value: 12.689396858215332 and parameters: {'k': 28, 'hid': 32, 'in_head': 8, 'out_head': 1, 'dor': 0.05, 'extra_layer': True}. Best is trial 3 with value: 10.104442596435547.\n",
      "[W 2024-03-13 23:25:26,584] Trial 6 failed with parameters: {'k': 2, 'hid': 512, 'in_head': 32, 'out_head': 1, 'dor': 0.05, 'extra_layer': False} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LinBoH\\AppData\\Local\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_12188\\1461073275.py\", line 52, in objective\n",
      "    loss = train()\n",
      "  File \"C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_12188\\1461073275.py\", line 46, in train\n",
      "    out = model(train_data.x, train_data.edge_index)\n",
      "  File \"C:\\Users\\LinBoH\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py\", line 1190, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\LinBoH\\AppData\\Local\\Temp\\ipykernel_12188\\4197177190.py\", line 22, in forward\n",
      "    x = F.elu(self.gat1(x, edge_index))\n",
      "  File \"C:\\Users\\LinBoH\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py\", line 1190, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"C:\\Users\\LinBoH\\AppData\\Local\\anaconda3\\lib\\site-packages\\torch_geometric\\nn\\conv\\gat_conv.py\", line 341, in forward\n",
      "    out = self.propagate(edge_index, x=x, alpha=alpha, size=size)\n",
      "  File \"C:\\Users\\LinBoH\\.cache\\pyg\\message_passing\\torch_geometric.nn.conv.gat_conv_GATConv_propagate.py\", line 161, in propagate\n",
      "    out = self.message(\n",
      "KeyboardInterrupt\n",
      "[W 2024-03-13 23:25:26,689] Trial 6 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 64\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m#     # Store the performance for each k\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m#     performance_history.append((k, test_loss))\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m#         best_performance = performance\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m#         best_k = k\u001b[39;00m\n\u001b[0;32m     63\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 64\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[18], line 52\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[1;32m---> 52\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[1;32mIn[18], line 46\u001b[0m, in \u001b[0;36mobjective.<locals>.train\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     45\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 46\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion_MAE(out, train_data\u001b[38;5;241m.\u001b[39my)\n\u001b[0;32m     48\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[17], line 22\u001b[0m, in \u001b[0;36mGATNet.forward\u001b[1;34m(self, x, edge_index)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index):\n\u001b[0;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdor, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[1;32m---> 22\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39melu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgat1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     23\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdor, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_layer:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\torch_geometric\\nn\\conv\\gat_conv.py:341\u001b[0m, in \u001b[0;36mGATConv.forward\u001b[1;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[0;32m    337\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_updater(edge_index, alpha\u001b[38;5;241m=\u001b[39malpha, edge_attr\u001b[38;5;241m=\u001b[39medge_attr,\n\u001b[0;32m    338\u001b[0m                           size\u001b[38;5;241m=\u001b[39msize)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: OptPairTensor, alpha: Tensor)\u001b[39;00m\n\u001b[1;32m--> 341\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpropagate\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcat:\n\u001b[0;32m    344\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_channels)\n",
      "File \u001b[1;32m~\\.cache\\pyg\\message_passing\\torch_geometric.nn.conv.gat_conv_GATConv_propagate.py:161\u001b[0m, in \u001b[0;36mpropagate\u001b[1;34m(self, edge_index, x, alpha, size)\u001b[0m\n\u001b[0;32m    152\u001b[0m             kwargs \u001b[38;5;241m=\u001b[39m CollectArgs(\n\u001b[0;32m    153\u001b[0m                 x_j\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx_j\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m    154\u001b[0m                 alpha\u001b[38;5;241m=\u001b[39mhook_kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m                 dim_size\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mdim_size,\n\u001b[0;32m    158\u001b[0m             )\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# End Message Forward Pre Hook #########################################\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_j\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_j\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# Begin Message Forward Hook ###########################################\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_compiling():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "best_k = None\n",
    "best_performance = float('inf')\n",
    "performance_history = []\n",
    "\n",
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    k = trial.suggest_int('k', 2, len(other_features))\n",
    "    hid = trial.suggest_categorical('hid', [16, 32, 64, 128, 256, 512])\n",
    "    in_head = trial.suggest_categorical('in_head', [1, 2, 4, 8, 16, 32])\n",
    "    out_head = trial.suggest_categorical('out_head', [1, 2])\n",
    "    dor = trial.suggest_categorical('dor', [0, 0.05, 0.1])\n",
    "    extra_layer = trial.suggest_categorical('extra_layer', [True, False])\n",
    "    \n",
    "    # Create a tensor of your labels/targets\n",
    "    y = torch.tensor(temp['remainder__link_counts'].values, dtype=torch.float).unsqueeze(1)\n",
    "    \n",
    "    # Feature selection for the current k\n",
    "    selector = SelectKBest(score_func=f_regression, k=k)\n",
    "    X_new = selector.fit_transform(temp[other_features], y)\n",
    "    selected_columns = list(temp[other_features].columns[selector.get_support(indices=True)])\n",
    "    \n",
    "    edge_index = torch.tensor(temp[nodes_features].values.T, dtype=torch.long)\n",
    "    x = torch.tensor(temp[selected_columns].values, dtype=torch.float)\n",
    "    data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "    test_edge_index = torch.tensor(test_data_tr[nodes_features].values.T, dtype=torch.long)\n",
    "    test_x = torch.tensor(test_data_tr[selected_columns].values, dtype=torch.float)\n",
    "    test_y = torch.tensor(test_data_tr['remainder__link_counts'].values, dtype=torch.float).unsqueeze(1)\n",
    "    test_data = Data(x=test_x, edge_index=test_edge_index, y=test_y)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    train_data = data.to(device)\n",
    "    test_data = test_data.to(device)\n",
    "    model = GATNet(k, 1, hid=hid, in_head=in_head, out_head=out_head, dor=dor, extra_layer=extra_layer).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "    criterion_MAE = torch.nn.L1Loss()\n",
    "    def train():\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(train_data.x, train_data.edge_index)\n",
    "        loss = criterion_MAE(out, train_data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        return loss\n",
    "    for epoch in range(20):\n",
    "        loss = train()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "#     # Store the performance for each k\n",
    "#     performance_history.append((k, test_loss))\n",
    "\n",
    "#     # Update the best k if the current performance is better\n",
    "#     if performance < best_performance:\n",
    "#         best_performance = performance\n",
    "#         best_k = k\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "best_k = best_params['k']\n",
    "best_hid = best_params['hid']\n",
    "best_in_head = best_params['in_head']\n",
    "best_out_head = best_params['out_head']\n",
    "best_dor = best_params['dor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection for the current k\n",
    "selector = SelectKBest(score_func=f_regression, k=best_k)\n",
    "y = torch.tensor(temp['remainder__link_counts'].values, dtype=torch.float).unsqueeze(1)\n",
    "X_new = selector.fit_transform(temp[other_features], y)\n",
    "selected_columns = list(temp[other_features].columns[selector.get_support(indices=True)])\n",
    "\n",
    "edge_index = torch.tensor(temp[nodes_features].values.T, dtype=torch.long)\n",
    "x = torch.tensor(temp[selected_columns].values, dtype=torch.float)\n",
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "test_edge_index = torch.tensor(test_data_tr[nodes_features].values.T, dtype=torch.long)\n",
    "test_x = torch.tensor(test_data_tr[selected_columns].values, dtype=torch.float)\n",
    "test_y = torch.tensor(test_data_tr['remainder__link_counts'].values, dtype=torch.float).unsqueeze(1)\n",
    "test_data = Data(x=test_x, edge_index=test_edge_index, y=test_y)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_data = data.to(device)\n",
    "test_data = test_data.to(device)\n",
    "\n",
    "best_model = GATNet(best_k, 1, hid=best_hid, in_head=best_in_head,\n",
    "                    out_head=best_out_head, dor=best_dor).to(device)\n",
    "optimizer = torch.optim.Adam(best_model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "criterion_MAE = torch.nn.L1Loss()\n",
    "def train():\n",
    "    best_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = best_model(train_data.x, train_data.edge_index)\n",
    "    loss = criterion_MAE(out, train_data.y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "for epoch in range(250):\n",
    "    loss = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11.932363510131836, 429.4022216796875)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion_MSE = torch.nn.MSELoss()\n",
    "def test(test_data):\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = best_model(test_data.x, test_data.edge_index)\n",
    "        loss_MAE = criterion_MAE(pred, test_data.y)\n",
    "        loss_MSE = criterion_MSE(pred, test_data.y)\n",
    "    return loss_MAE.item(), loss_MSE.item()\n",
    "\n",
    "test_loss = test(test_data)\n",
    "test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase, basic_auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = GraphDatabase.driver(\n",
    "  \"bolt://100.24.59.192:7687\",\n",
    "  auth=basic_auth(\"neo4j\", \"radiuses-vision-seasons\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(driver, df):\n",
    "    # Convert DataFrame to a list of dictionaries\n",
    "    rows = df.to_dict('records')\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        # Use UNWIND to process each row as a separate Cypher operation\n",
    "        session.run(\"\"\"\n",
    "            UNWIND $rows AS row\n",
    "            MERGE (start:Location {x: row.num_preprocess__start_node_x, y: row.num_preprocess__start_node_y})\n",
    "            MERGE (end:Location {x: row.num_preprocess__end_node_x, y: row.num_preprocess__end_node_y})\n",
    "            MERGE (start)-[r:LINK]->(end)\n",
    "            SET r += {\n",
    "                length: row.num_preprocess__link_length,\n",
    "                freespeed: row.num_preprocess__link_freespeed,\n",
    "                capacity: row.num_preprocess__link_capacity,\n",
    "                permlanes: row.num_preprocess__link_permlanes,\n",
    "                start_count: row.num_preprocess__start_count,\n",
    "                end_count: row.num_preprocess__end_count,\n",
    "                link_id: row.remainder__link_id,\n",
    "                link_counts: row.remainder__link_counts,\n",
    "                dataset: row.remainder__dataset,\n",
    "                used_count: row.remainder__used_count,\n",
    "                x_y_coor: row.remainder__x_y_coor,\n",
    "                similar_link: row.remainder__similar_link,\n",
    "                planxml: row.remainder__planxml\n",
    "            }\n",
    "        \"\"\", parameters={'rows': rows})\n",
    "df2 = temp.head(10)\n",
    "import_data(driver, df2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gds_graph(driver):\n",
    "    with driver.session() as session:\n",
    "        session.run(\"\"\"\n",
    "            CALL gds.graph.project(\n",
    "                'roadGraph',\n",
    "                'Location',\n",
    "                'LINK',\n",
    "                {\n",
    "                    nodeProperties: ['x', 'y'],\n",
    "                    relationshipProperties: [\n",
    "                        'length', 'freespeed', 'capacity', 'permlanes',\n",
    "                        'start_count', 'end_count', 'link_id', 'dataset',\n",
    "                        'used_count', 'x_y_coor', 'similar_link', 'planxml',\n",
    "                        'link_counts'\n",
    "                    ]\n",
    "                }\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "create_gds_graph(driver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BoltDriver' object has no attribute 'beta'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pipe, _ \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[38;5;241m.\u001b[39mpipeline\u001b[38;5;241m.\u001b[39mnodeClassification\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlink-pipe\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m pipe\u001b[38;5;241m.\u001b[39mconfigureSplit(testFraction\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, validationFolds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BoltDriver' object has no attribute 'beta'"
     ]
    }
   ],
   "source": [
    "pipe, _ = driver.beta.pipeline.nodeClassification.create(\"link-pipe\")\n",
    "pipe.configureSplit(testFraction=0.2, validationFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gnn_model(driver):\n",
    "    with driver.session() as session:\n",
    "        session.run(\"\"\"\n",
    "                CALL gds.beta.graphSage.train(\n",
    "                  'roadGraph', \n",
    "                  {\n",
    "                    modelName: 'graphSageModel',\n",
    "                    featureProperties: [\n",
    "                      'length', 'freespeed', 'capacity', 'permlanes'\n",
    "                    ],\n",
    "                    targetProperty: 'link_counts', \n",
    "                    hiddenLayerSizes: [256, 128],\n",
    "                    sampleSizes: [25, 10],\n",
    "                    batchSize: 32,\n",
    "                    epochs: 10,\n",
    "                    learningRate: 0.01\n",
    "                  }\n",
    "                )\n",
    "                YIELD modelInfo\n",
    "        \"\"\")\n",
    "\n",
    "train_gnn_model(driver)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "{code: Neo.ClientError.Procedure.ProcedureCallFailed} {message: Failed to invoke procedure `gds.beta.graphSage.write`: Caused by: java.util.NoSuchElementException: Model with name `graphSageModel` does not exist.}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[0;32m     14\u001b[0m             \u001b[38;5;28mprint\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnodeId\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictedClass\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 16\u001b[0m \u001b[43mpredict_link_counts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[70], line 13\u001b[0m, in \u001b[0;36mpredict_link_counts\u001b[1;34m(driver)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m driver\u001b[38;5;241m.\u001b[39msession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m      3\u001b[0m     result \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124m            CALL gds.beta.graphSage.write(\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124m              \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroadGraph\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124m            YIELD nodePropertiesWritten\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28mprint\u001b[39m(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnodeId\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictedClass\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\neo4j\\_sync\\work\\result.py:270\u001b[0m, in \u001b[0;36mResult.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_record_buffer\u001b[38;5;241m.\u001b[39mpopleft()\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streaming:\n\u001b[1;32m--> 270\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_discarding:\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_discard()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\neo4j\\_sync\\io\\_common.py:178\u001b[0m, in \u001b[0;36mConnectionErrorHandler.__getattr__.<locals>.outer.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 178\u001b[0m         func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (Neo4jError, ServiceUnavailable, SessionExpired) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    180\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39miscoroutinefunction(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__on_error)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\neo4j\\_sync\\io\\_bolt.py:849\u001b[0m, in \u001b[0;36mBolt.fetch_message\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    845\u001b[0m \u001b[38;5;66;03m# Receive exactly one message\u001b[39;00m\n\u001b[0;32m    846\u001b[0m tag, fields \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minbox\u001b[38;5;241m.\u001b[39mpop(\n\u001b[0;32m    847\u001b[0m     hydration_hooks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponses[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhydration_hooks\n\u001b[0;32m    848\u001b[0m )\n\u001b[1;32m--> 849\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midle_since \u001b[38;5;241m=\u001b[39m monotonic()\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\neo4j\\_sync\\io\\_bolt5.py:369\u001b[0m, in \u001b[0;36mBolt5x0._process_message\u001b[1;34m(self, tag, fields)\u001b[0m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_server_state_manager\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbolt_states\u001b[38;5;241m.\u001b[39mFAILED\n\u001b[0;32m    368\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 369\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary_metadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ServiceUnavailable, DatabaseUnavailable):\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\neo4j\\_sync\\io\\_common.py:245\u001b[0m, in \u001b[0;36mResponse.on_failure\u001b[1;34m(self, metadata)\u001b[0m\n\u001b[0;32m    243\u001b[0m handler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandlers\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_summary\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    244\u001b[0m Util\u001b[38;5;241m.\u001b[39mcallback(handler)\n\u001b[1;32m--> 245\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m Neo4jError\u001b[38;5;241m.\u001b[39mhydrate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmetadata)\n",
      "\u001b[1;31mClientError\u001b[0m: {code: Neo.ClientError.Procedure.ProcedureCallFailed} {message: Failed to invoke procedure `gds.beta.graphSage.write`: Caused by: java.util.NoSuchElementException: Model with name `graphSageModel` does not exist.}"
     ]
    }
   ],
   "source": [
    "def predict_link_counts(driver):\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"\"\"\n",
    "                CALL gds.beta.graphSage.write(\n",
    "                  'roadGraph',\n",
    "                  {\n",
    "                    modelName: 'graphSageModel',\n",
    "                    writeProperty: 'predictedLinkCounts'\n",
    "                  }\n",
    "                )\n",
    "                YIELD nodePropertiesWritten\n",
    "        \"\"\")\n",
    "        for row in result:\n",
    "            print(row['nodeId'], row['predictedClass'])\n",
    "\n",
    "predict_link_counts(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
